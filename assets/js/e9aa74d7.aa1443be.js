"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[22033],{15680:(e,n,t)=>{t.d(n,{xA:()=>c,yg:()=>g});var r=t(96540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,r,i=function(e,n){if(null==e)return{};var t,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var p=r.createContext({}),l=function(e){var n=r.useContext(p),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},c=function(e){var n=l(e.components);return r.createElement(p.Provider,{value:n},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},m=r.forwardRef((function(e,n){var t=e.components,i=e.mdxType,a=e.originalType,p=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=l(t),m=i,g=u["".concat(p,".").concat(m)]||u[m]||d[m]||a;return t?r.createElement(g,o(o({ref:n},c),{},{components:t})):r.createElement(g,o({ref:n},c))}));function g(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var a=t.length,o=new Array(a);o[0]=m;var s={};for(var p in n)hasOwnProperty.call(n,p)&&(s[p]=n[p]);s.originalType=e,s[u]="string"==typeof e?e:i,o[1]=s;for(var l=2;l<a;l++)o[l]=t[l];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}m.displayName="MDXCreateElement"},70348:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var r=t(9668),i=(t(96540),t(15680));const a={description:"LLM - integrated with Raspberry Pi5",title:"ChatGPT - Raspberry Pi",keywords:["Raspberry Pi","LLM","ReSpeaker"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/respeaker_lite_pi5",last_update:{date:"8/12/2024",author:"ShuishengPeng"}},o=void 0,s={unversionedId:"Sensor/ReSpeaker_Lite/Application/respeaker_lite_pi5",id:"Sensor/ReSpeaker_Lite/Application/respeaker_lite_pi5",title:"ChatGPT - Raspberry Pi",description:"LLM - integrated with Raspberry Pi5",source:"@site/docs/Sensor/ReSpeaker_Lite/Application/respeaker_lite_pi5.md",sourceDirName:"Sensor/ReSpeaker_Lite/Application",slug:"/respeaker_lite_pi5",permalink:"/respeaker_lite_pi5",draft:!1,editUrl:"https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Sensor/ReSpeaker_Lite/Application/respeaker_lite_pi5.md",tags:[],version:"current",lastUpdatedBy:"ShuishengPeng",lastUpdatedAt:1723420800,formattedLastUpdatedAt:"Aug 12, 2024",frontMatter:{description:"LLM - integrated with Raspberry Pi5",title:"ChatGPT - Raspberry Pi",keywords:["Raspberry Pi","LLM","ReSpeaker"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/respeaker_lite_pi5",last_update:{date:"8/12/2024",author:"ShuishengPeng"}},sidebar:"ProductSidebar",previous:{title:"Voice Assistant System for Home Assitant",permalink:"/respeaker_lite_ha"},next:{title:"Ultra Sonic range measurement module",permalink:"/Ultra_Sonic_range_measurement_module"}},p={},l=[{value:"Hardware Required",id:"hardware-required",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Install Libraries",id:"install-libraries",level:3},{value:"Code",id:"code",level:3}],c={toc:l},u="wrapper";function d(e){let{components:n,...t}=e;return(0,i.yg)(u,(0,r.A)({},c,t,{components:n,mdxType:"MDXLayout"}),(0,i.yg)("p",null,"This project integrates voice input, large model response, and voice output functionalities using a Raspberry Pi 5. It employs the ReSpeaker Lite as the audio input and output device, enabling seamless interaction with ChatGPT and speech-to-text conversion services."),(0,i.yg)("div",{class:"table-center"},(0,i.yg)("iframe",{width:"730",height:"500",src:"https://files.seeedstudio.com/wiki/SenseCAP/respeaker/pi.mp4",scrolling:"no",border:"0",frameborder:"no",framespacing:"0",allowfullscreen:"true"}," ")),(0,i.yg)("h2",{id:"hardware-required"},"Hardware Required"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("a",{parentName:"li",href:"https://www.seeedstudio.com/ReSpeaker-Lite-p-5928.html"},"ReSpeaker Lite USB 2-Mic Array")),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("a",{parentName:"li",href:"https://www.seeedstudio.com/Raspberry-Pi-5-8GB-p-5810.html"},"Raspberry Pi 5"))),(0,i.yg)("h2",{id:"getting-started"},"Getting Started"),(0,i.yg)("p",null,"Check the ",(0,i.yg)("a",{parentName:"p",href:"https://www.raspberrypi.com/documentation/computers/getting-started.html#getting-started-with-your-raspberry-pi"},"Getting started documentation")," to set up your Raspberry Pi first, connect your Pi to the network."),(0,i.yg)("admonition",{type:"note"},(0,i.yg)("p",{parentName:"admonition"},"Make sure your python version is newer than python3.7.1.",(0,i.yg)("br",null),"\nTo check the version:"),(0,i.yg)("pre",{parentName:"admonition"},(0,i.yg)("code",{parentName:"pre"},"python3 --version\n"))),(0,i.yg)("h3",{id:"install-libraries"},"Install Libraries"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-shell"},"sudo apt update\nsudo apt install python3-pip python3-dev\nsudo apt install portaudio19-dev\npip3 install pyaudio\npip3 install speechrecognition\npip3 install openai\npip3 install playsound\n")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"For Raspberry Pi 5, run the following command to configure ReSpeaker Lite:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-shell"},"pw-metadata -n settings 0 clock.force-rate 16000\n")),(0,i.yg)("p",null,"For a permanent change, un-hash and edit ",(0,i.yg)("inlineCode",{parentName:"p"},"default.clock.rate")," line in ",(0,i.yg)("inlineCode",{parentName:"p"},"/etc/pipewire/pipewire.conf")," (copy it first from /usr/share/)."),(0,i.yg)("admonition",{type:"tip"},(0,i.yg)("p",{parentName:"admonition"},"Command to adjust the volume of ReSpeaker Lite:"),(0,i.yg)("pre",{parentName:"admonition"},(0,i.yg)("code",{parentName:"pre",className:"language-shell"},"alsamixer\n"))),(0,i.yg)("h3",{id:"code"},"Code"),(0,i.yg)("p",null,"This Python code implements a simple voice assistant that listens for a wake word, recognizes user voice commands, converts them to text, generates a response using ",(0,i.yg)("inlineCode",{parentName:"p"},"GPT-4"),", and then converts the response to speech and plays it back."),(0,i.yg)("p",null,"The device first waits for the wake word, then listens for the user's command. Once the command is received, the program generates a response using GPT-4 and plays it back as speech. If it fails to recognize the command three times, it returns to listening for the wake word, you'll need to say the wake word again to initiate a new voice interaction session."),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Step1"),": Configure API key")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-shell"},"export OPENAI_API_KEY= 'your-api-key-here'\n")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Step2"),": Create a new python file and enter the following code:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},'import speech_recognition as sr\nfrom openai import OpenAI\nfrom pathlib import Path\nfrom pydub import AudioSegment\nimport os\n\n\nclient = OpenAI()\n\ndef text_to_speech(text):\n    speech_file_path = Path(__file__).parent / "speech.mp3"\n    response = client.audio.speech.create(\n    model="tts-1",\n    voice="alloy",\n    input=text\n    )\n\n    response.stream_to_file(speech_file_path)\n    audio = AudioSegment.from_mp3("speech.mp3")\n    audio.export("speech.wav", format="wav")\n    cmdline = \'aplay \' + " speech.wav" \n    os.system(cmdline)\n\n\n\n# Initialize recognizer\nrecognizer = sr.Recognizer()\nmicrophone = sr.Microphone()\n\n# Define the wake word\nWAKE_WORD = "hi"\n\ndef listen_for_wake_word():\n    with microphone as source:\n        recognizer.adjust_for_ambient_noise(source, duration=0.5)\n        print("Listening for wake word...")\n        \n        while True:\n            audio = recognizer.listen(source)\n            # audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)\n            try:\n                text = recognizer.recognize_google(audio).lower()\n                if WAKE_WORD in text:\n                    print(f"Wake word \'{WAKE_WORD}\' detected.")\n                    text_to_speech("hi,what can i do for you?")\n                    return True\n            except sr.UnknownValueError:\n                continue\n            except sr.RequestError as e:\n                print(f"Could not request results; {e}")\n                continue\n\ni=0\ndef listen_for_command():\n    global i\n    with microphone as source:\n        print("Listening for command...")\n        # audio = recognizer.listen(source)\n        audio = recognizer.listen(source, timeout=5, phrase_time_limit=5)\n        try:\n            command = recognizer.recognize_google(audio)\n            print(f"You said: {command}")\n            i=0\n            return command\n        except sr.UnknownValueError:\n            print("Could not understand the audio")\n            i = i+1\n        except sr.RequestError as e:\n            print(f"Could not request results; {e}")\n            i = i+1\n\n\ndef get_gpt_response(prompt):\n    completion = client.chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[\n        {"role": "system", "content": "Your name is speaker, you can answer all kinds of questions for me"},\n        {"role": "user", "content": prompt}\n    ]\n    )\n\n    content_string = completion.choices[0].message.content\n    paragraphs = content_string.split(\'\\n\\n\')\n    combined_content = \' \'.join(paragraphs)\n    return combined_content\n\n\n\n\ndef main():\n    global i\n    while 1:\n        flag = listen_for_wake_word()\n        while flag == True:\n            user_input = listen_for_command()\n            if i==3:\n                flag = False\n                i = 0\n            if user_input:\n                gpt_response = get_gpt_response(user_input)\n                print(f"GPT says: {gpt_response}")\n                text_to_speech(gpt_response)\n                \n\nif __name__ == "__main__":\n    main()\n')),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Step3"),": Run the python file.")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-shell"},"python openai.py\n")),(0,i.yg)("p",null,"Now you are all set, try waking it up with ",(0,i.yg)("inlineCode",{parentName:"p"},"Hi")," and talking to it!"),(0,i.yg)("div",{class:"table-center"},(0,i.yg)("iframe",{width:"730",height:"500",src:"https://files.seeedstudio.com/wiki/SenseCAP/respeaker/pi.mp4",scrolling:"no",border:"0",frameborder:"no",framespacing:"0",allowfullscreen:"true"}," ")))}d.isMDXComponent=!0}}]);