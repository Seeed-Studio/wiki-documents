"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[70618],{15680:(e,t,o)=>{o.d(t,{xA:()=>p,yg:()=>c});var a=o(96540);function n(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function i(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,a)}return o}function l(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?i(Object(o),!0).forEach((function(t){n(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):i(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function r(e,t){if(null==e)return{};var o,a,n=function(e,t){if(null==e)return{};var o,a,n={},i=Object.keys(e);for(a=0;a<i.length;a++)o=i[a],t.indexOf(o)>=0||(n[o]=e[o]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)o=i[a],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(n[o]=e[o])}return n}var s=a.createContext({}),g=function(e){var t=a.useContext(s),o=t;return e&&(o="function"==typeof e?e(t):l(l({},t),e)),o},p=function(e){var t=g(e.components);return a.createElement(s.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},y=a.forwardRef((function(e,t){var o=e.components,n=e.mdxType,i=e.originalType,s=e.parentName,p=r(e,["components","mdxType","originalType","parentName"]),d=g(o),y=n,c=d["".concat(s,".").concat(y)]||d[y]||u[y]||i;return o?a.createElement(c,l(l({ref:t},p),{},{components:o})):a.createElement(c,l({ref:t},p))}));function c(e,t){var o=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=o.length,l=new Array(i);l[0]=y;var r={};for(var s in t)hasOwnProperty.call(t,s)&&(r[s]=t[s]);r.originalType=e,r[d]="string"==typeof e?e:n,l[1]=r;for(var g=2;g<i;g++)l[g]=o[g];return a.createElement.apply(null,l)}return a.createElement.apply(null,o)}y.displayName="MDXCreateElement"},97744:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>g});var a=o(58168),n=(o(96540),o(15680));const i={description:"Train and Deploy Your Own AI Model with Roboflow, YOLOv5, TensorFlow Lite",title:"Train and Deploy Your Own AI Model with Roboflow, YOLOv5, TensorFlow Lite",keywords:["Sensor Vision_AI"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/Train-Deploy-AI-Model-Grove-Vision-AI",last_update:{date:"1/13/2023",author:"shuxu hu"}},l="Train and Deploy Your Own AI Model Into Grove - Vision AI",r={unversionedId:"Sensor/Grove/Grove_Sensors/AI-powered/Train-Deploy-AI-Model-Grove-Vision-AI",id:"Sensor/Grove/Grove_Sensors/AI-powered/Train-Deploy-AI-Model-Grove-Vision-AI",title:"Train and Deploy Your Own AI Model with Roboflow, YOLOv5, TensorFlow Lite",description:"Train and Deploy Your Own AI Model with Roboflow, YOLOv5, TensorFlow Lite",source:"@site/docs/Sensor/Grove/Grove_Sensors/AI-powered/Train-Deploy-AI-Model-Grove-Vision-AI.md",sourceDirName:"Sensor/Grove/Grove_Sensors/AI-powered",slug:"/Train-Deploy-AI-Model-Grove-Vision-AI",permalink:"/Train-Deploy-AI-Model-Grove-Vision-AI",draft:!1,editUrl:"https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Sensor/Grove/Grove_Sensors/AI-powered/Train-Deploy-AI-Model-Grove-Vision-AI.md",tags:[],version:"current",lastUpdatedBy:"shuxu hu",lastUpdatedAt:1673568e3,formattedLastUpdatedAt:"Jan 13, 2023",frontMatter:{description:"Train and Deploy Your Own AI Model with Roboflow, YOLOv5, TensorFlow Lite",title:"Train and Deploy Your Own AI Model with Roboflow, YOLOv5, TensorFlow Lite",keywords:["Sensor Vision_AI"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/Train-Deploy-AI-Model-Grove-Vision-AI",last_update:{date:"1/13/2023",author:"shuxu hu"}},sidebar:"ProductSidebar",previous:{title:"Grove Vision AI Module",permalink:"/Grove-Vision-AI-Module"},next:{title:"Train and Deploy Your Own AI Model with Edge Impulse",permalink:"/edge-impulse-vision-ai"}},s={},g=[{value:"Upgradable to Industrial Sensors",id:"upgradable-to-industrial-sensors",level:2},{value:"Overview",id:"overview",level:2},{value:"Hardware introduction",id:"hardware-introduction",level:2},{value:"Grove - Vision AI Module",id:"grove---vision-ai-module",level:3},{value:"Software introduction",id:"software-introduction",level:2},{value:"What is Roboflow?",id:"what-is-roboflow",level:3},{value:"What is YOLOv5?",id:"what-is-yolov5",level:3},{value:"What is TensorFlow Lite?",id:"what-is-tensorflow-lite",level:3},{value:"Wiki structure",id:"wiki-structure",level:2},{value:'<span id="jump1">1. Train your own AI model with a public dataset</span>',id:"1-train-your-own-ai-model-with-a-public-dataset",level:2},{value:"Hardware preparation",id:"hardware-preparation",level:3},{value:"Software preparation",id:"software-preparation",level:3},{value:"Use publicly available annotated dataset",id:"use-publicly-available-annotated-dataset",level:3},{value:"Train using YOLOv5 on Google Colab",id:"train-using-yolov5-on-google-colab",level:3},{value:"Deploy and inference",id:"deploy-and-inference",level:3},{value:'<span id="jump2">2. Train your own AI model with your own dataset</span>',id:"2-train-your-own-ai-model-with-your-own-dataset",level:2},{value:"Annotate dataset using Roboflow",id:"annotate-dataset-using-roboflow",level:3},{value:"Train using YOLOv5 on Google Colab",id:"train-using-yolov5-on-google-colab-1",level:3},{value:'<span id="jump3">3. Deploy the trained model and perform inference</span>',id:"3-deploy-the-trained-model-and-perform-inference",level:2},{value:"Grove - Vision AI Module",id:"grove---vision-ai-module-1",level:3},{value:"Bonus content",id:"bonus-content",level:2},{value:"Can I train an AI model on my PC?",id:"can-i-train-an-ai-model-on-my-pc",level:3},{value:"Resources",id:"resources",level:2},{value:"Tech Support &amp; Product Discussion",id:"tech-support--product-discussion",level:2}],p={toc:g},d="wrapper";function u(e){let{components:t,...o}=e;return(0,n.yg)(d,(0,a.A)({},p,o,{components:t,mdxType:"MDXLayout"}),(0,n.yg)("h1",{id:"train-and-deploy-your-own-ai-model-into-grove---vision-ai"},"Train and Deploy Your Own AI Model Into Grove - Vision AI"),(0,n.yg)("h2",{id:"upgradable-to-industrial-sensors"},"Upgradable to Industrial Sensors"),(0,n.yg)("p",null,"With the SenseCAP ",(0,n.yg)("a",{parentName:"p",href:"https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html"},"S2110 controller")," and ",(0,n.yg)("a",{parentName:"p",href:"https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html"},"S2100 data logger"),", you can easily turn the Grove into a LoRaWAN\xae sensor. Seeed not only helps you with prototyping but also offers you the possibility to expand your project with the SenseCAP series of robust ",(0,n.yg)("a",{parentName:"p",href:"https://www.seeedstudio.com/catalogsearch/result/?q=sensecap&categories=SenseCAP&application=Temperature%2FHumidity~Soil~Gas~Light~Weather~Water~Automation~Positioning~Machine%20Learning~Voice%20Recognition&compatibility=SenseCAP"},"industrial sensors"),"."),(0,n.yg)("p",null,"The IP66 housing, Bluetooth configuration, compatibility with the global LoRaWAN\xae network, built-in 19 Ah battery, and powerful support from APP make the ",(0,n.yg)("a",{parentName:"p",href:"https://www.seeedstudio.com/catalogsearch/result/?q=S21&categories=SenseCAP~LoRaWAN%20Device&product_module=Device"},"SenseCAP S210x")," the best choice for industrial applications. The series includes sensors for soil moisture, air temperature and humidity, light intensity, CO2, EC, and an 8-in-1 weather station. Try the latest SenseCAP S210x for your next successful industrial project."),(0,n.yg)("table",{style:{marginLeft:"auto",marginRight:"auto"}},(0,n.yg)("tbody",null,(0,n.yg)("tr",null,(0,n.yg)("td",{colSpan:4,bgcolor:"#0e3c49",align:"center"},(0,n.yg)("font",{color:"white",size:4},(0,n.yg)("strong",null,"SenseCAP Industrial Sensor")))),(0,n.yg)("tr",null,(0,n.yg)("td",{bgcolor:"#0e3c49"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html",target:"_blank"}),(0,n.yg)("div",{align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html",target:"_blank"},(0,n.yg)("img",{width:"100%",src:"https://files.seeedstudio.com/wiki/K1100_overview/2/S2100.png"})))),(0,n.yg)("td",{bgcolor:"#0e3c49"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2101-LoRaWAN-Air-Temperature-and-Humidity-Sensor-p-5354.html",target:"_blank"}),(0,n.yg)("div",{align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2101-LoRaWAN-Air-Temperature-and-Humidity-Sensor-p-5354.html",target:"_blank"},(0,n.yg)("img",{width:"100%",src:"https://files.seeedstudio.com/wiki/K1100_overview/2/S2101&S2103.png"})))),(0,n.yg)("td",{bgcolor:"#0e3c49"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2102-LoRaWAN-Light-Intensity-Sensor-p-5355.html",target:"_blank"}),(0,n.yg)("div",{align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2102-LoRaWAN-Light-Intensity-Sensor-p-5355.html",target:"_blank"},(0,n.yg)("img",{width:"100%",src:"https://files.seeedstudio.com/wiki/K1100_overview/2/S2102.png"})))),(0,n.yg)("td",{bgcolor:"#0e3c49"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2103-LoRaWAN-CO2-Temperature-and-Humidity-Sensor-p-5356.html",target:"_blank"}),(0,n.yg)("div",{align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2103-LoRaWAN-CO2-Temperature-and-Humidity-Sensor-p-5356.html",target:"_blank"},(0,n.yg)("img",{width:"100%",src:"https://files.seeedstudio.com/wiki/K1100_overview/2/S2101&S2103.png"}))))),(0,n.yg)("tr",null,(0,n.yg)("td",{bgcolor:"#0e3c49",align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html",target:"_blank"},(0,n.yg)("strong",null,"S2100 ",(0,n.yg)("br",null)," Data Logger"))),(0,n.yg)("td",{bgcolor:"#0e3c49",align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2101-LoRaWAN-Air-Temperature-and-Humidity-Sensor-p-5354.html",target:"_blank"},(0,n.yg)("strong",null,"S2101 ",(0,n.yg)("br",null)," Air Temp & Humidity"))),(0,n.yg)("td",{bgcolor:"#0e3c49",align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2102-LoRaWAN-Light-Intensity-Sensor-p-5355.html",target:"_blank"},(0,n.yg)("strong",null,"S2102 ",(0,n.yg)("br",null)," Light"))),(0,n.yg)("td",{bgcolor:"#0e3c49",align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2103-LoRaWAN-CO2-Temperature-and-Humidity-Sensor-p-5356.html",target:"_blank"},(0,n.yg)("strong",null,"S2103 ",(0,n.yg)("br",null)," Air Temp & Humidity & CO2")))),(0,n.yg)("tr",null,(0,n.yg)("td",{bgcolor:"#0e3c49"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2104-LoRaWAN-Soil-Temperature-and-Moisture-Sensor-p-5357.html",target:"_blank"}),(0,n.yg)("div",{align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2104-LoRaWAN-Soil-Temperature-and-Moisture-Sensor-p-5357.html",target:"_blank"},(0,n.yg)("img",{width:"100%",src:"https://files.seeedstudio.com/wiki/K1100_overview/2/S2104.png"})))),(0,n.yg)("td",{bgcolor:"#0e3c49"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2105-LoRaWAN-Soil-Temperature-Moisture-and-EC-Sensor-p-5358.html",target:"_blank"}),(0,n.yg)("div",{align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2105-LoRaWAN-Soil-Temperature-Moisture-and-EC-Sensor-p-5358.html",target:"_blank"},(0,n.yg)("img",{width:"100%",src:"https://files.seeedstudio.com/wiki/K1100_overview/2/S2105.png"})))),(0,n.yg)("td",{bgcolor:"#0e3c49"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html",target:"_blank"}),(0,n.yg)("div",{align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html",target:"_blank"},(0,n.yg)("img",{width:"100%",src:"https://files.seeedstudio.com/wiki/K1100_overview/2/S2110.png"})))),(0,n.yg)("td",{bgcolor:"#0e3c49"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/sensecap-s2120-lorawan-8-in-1-weather-sensor-p-5436.html",target:"_blank"}),(0,n.yg)("div",{align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/sensecap-s2120-lorawan-8-in-1-weather-sensor-p-5436.html",target:"_blank"},(0,n.yg)("img",{width:"100%",src:"https://files.seeedstudio.com/wiki/K1100_overview/2/S2120.png"}))))),(0,n.yg)("tr",null,(0,n.yg)("td",{bgcolor:"#0e3c49",align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2104-LoRaWAN-Soil-Temperature-and-Moisture-Sensor-p-5357.html",target:"_blank"},(0,n.yg)("strong",null,"S2104 ",(0,n.yg)("br",null)," Soil Moisture & Temp"))),(0,n.yg)("td",{bgcolor:"#0e3c49",align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-S2105-LoRaWAN-Soil-Temperature-Moisture-and-EC-Sensor-p-5358.html",target:"_blank"},(0,n.yg)("strong",null,"S2105 ",(0,n.yg)("br",null)," Soil Moisture & Temp & EC"))),(0,n.yg)("td",{bgcolor:"#0e3c49",align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html",target:"_blank"},(0,n.yg)("strong",null,"S2110 ",(0,n.yg)("br",null)," LoRaWAN\xae Controller"))),(0,n.yg)("td",{bgcolor:"#0e3c49",align:"center"},(0,n.yg)("a",{href:"https://www.seeedstudio.com/sensecap-s2120-lorawan-8-in-1-weather-sensor-p-5436.html",target:"_blank"},(0,n.yg)("strong",null,"S2120 ",(0,n.yg)("br",null)," 8-in-1 Weather Station")))))),(0,n.yg)("h2",{id:"overview"},"Overview"),(0,n.yg)("p",null,"In this wiki, we will teach you how to train your own AI model for your specific application and then deploy it easily to the Grove - Vision AI Module. Let's get started!"),(0,n.yg)("h2",{id:"hardware-introduction"},"Hardware introduction"),(0,n.yg)("p",null,"We will mainly use the Grove - Vision AI Module throughout this wiki. So first, let's become familiar with the hardware."),(0,n.yg)("h3",{id:"grove---vision-ai-module"},"Grove - Vision AI Module"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://www.seeedstudio.com/Grove-Vision-AI-Module-p-5457.html"},"Grove Vision AI Module")," represents a thumb-sized AI camera, customized sensor which has been already installed ML algorithm for people detection, and other customized models. Being easily deployed and displayed within minutes, it works under ultra-low power model, and provides two ways of singal transmission and multiple onboard modules, all of which make it perfect for getting started with AI-powered camera."),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:350,src:"https://files.seeedstudio.com/wiki/Wio-Terminal-Developer-for-helium/camera.jpg"})),(0,n.yg)("h2",{id:"software-introduction"},"Software introduction"),(0,n.yg)("p",null,"We will be using the following software technologies in this wiki"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Roboflow - for annotating"),(0,n.yg)("li",{parentName:"ul"},"YOLOv5 - for training"),(0,n.yg)("li",{parentName:"ul"},"TensorFlow Lite - for inferencing")),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:600,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/57.png"})),(0,n.yg)("h3",{id:"what-is-roboflow"},"What is Roboflow?"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://roboflow.com"},"Roboflow")," is an annotation tool based online. This tool allows you to easily annotate all your images, add further processing to these images and export the labeled dataset into different formats such as YOLOV5 PyTorch, Pascal VOC, and more! Roboflow also has public datasets readily available to users."),(0,n.yg)("h3",{id:"what-is-yolov5"},"What is YOLOv5?"),(0,n.yg)("p",null,"YOLO is an abbreviation for the term \u2018You Only Look Once\u2019. It is an algorithm that detects and recognizes various objects in an image in real-time. Ultralytics ",(0,n.yg)("a",{parentName:"p",href:"https://ultralytics.com/yolov5"},"YOLOv5")," is the version of YOLO based on the PyTorch framework."),(0,n.yg)("h3",{id:"what-is-tensorflow-lite"},"What is TensorFlow Lite?"),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://www.tensorflow.org/lite"},"TensorFlow Lite")," is an open-source, product ready, cross-platform deep learning framework that converts a pre-trained model in TensorFlow to a special format that can be optimized for speed or storage. The special format model can be deployed on edge devices like mobiles using Android or iOS or Linux based embedded devices like Raspberry Pi or Microcontrollers to make the inference at the Edge."),(0,n.yg)("h2",{id:"wiki-structure"},"Wiki structure"),(0,n.yg)("p",null,"This wiki will be divided into three main sections"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"#jump1"},"Train your own AI model with a public dataset")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"#jump2"},"Train your own AI model with your own dataset")),(0,n.yg)("li",{parentName:"ol"},(0,n.yg)("a",{parentName:"li",href:"#jump3"},"Deploy the trained AI model into Grove - Vision AI Module"))),(0,n.yg)("p",null,"The first section will be the fastest way to build your own AI model with the least number of steps. The second section will take some time and effort to build your own AI model, but it will be definitely worth the knowledge. The third section about deploying the AI model can be done either after first or second section."),(0,n.yg)("p",null,"So there are two ways to follow this wiki:"),(0,n.yg)("ol",null,(0,n.yg)("li",{parentName:"ol"},"Follow ",(0,n.yg)("a",{parentName:"li",href:"#jump1"},"section 1")," and then ",(0,n.yg)("a",{parentName:"li",href:"#jump3"},"section 3")," - fast to follow"),(0,n.yg)("li",{parentName:"ol"},"Follow ",(0,n.yg)("a",{parentName:"li",href:"#jump2"},"section 2")," and then ",(0,n.yg)("a",{parentName:"li",href:"#jump3"},"section 3")," - slow to follow")),(0,n.yg)("p",null,"However, we encourage to follow the first way at first and then move onto the second way."),(0,n.yg)("h2",{id:"1-train-your-own-ai-model-with-a-public-dataset"},(0,n.yg)("span",{id:"jump1"},"1. Train your own AI model with a public dataset")),(0,n.yg)("p",null,"The very first step of an object detection project is to obtain data for training. You can either download datasets available publicly or create your own dataset!"),(0,n.yg)("p",null,"But what is the fastest and easiest way to get started with object detection? Well...Using public datasets can save you a lot of time that you would otherwise spend on collecting data by yourself and annotating them. These public datasets are already annotated out-of-the-box, giving you more time to focus on your AI vision applications."),(0,n.yg)("h3",{id:"hardware-preparation"},"Hardware preparation"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Grove - Vision AI Module"),(0,n.yg)("li",{parentName:"ul"},"USB Type-C cable"),(0,n.yg)("li",{parentName:"ul"},"Windows/ Linux/ Mac with internet access")),(0,n.yg)("h3",{id:"software-preparation"},"Software preparation"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"No need to prepare additional software")),(0,n.yg)("h3",{id:"use-publicly-available-annotated-dataset"},"Use publicly available annotated dataset"),(0,n.yg)("p",null,"You can download a number of publically available datasets such as the  ",(0,n.yg)("a",{parentName:"p",href:"https://cocodataset.org"},"COCO dataset"),", ",(0,n.yg)("a",{parentName:"p",href:"http://host.robots.ox.ac.uk/pascal/VOC"},"Pascal VOC dataset")," and much more. ",(0,n.yg)("a",{parentName:"p",href:"https://universe.roboflow.com"},"Roboflow Universe")," is a recommended platform which provides a wide-range of datasets and it has ",(0,n.yg)("a",{parentName:"p",href:"https://blog.roboflow.com/computer-vision-datasets-and-apis"},"90,000+ datasets with 66+ million images")," available for building computer vision models. Also, you can simply search ",(0,n.yg)("strong",{parentName:"p"},"open-source datasets")," on Google and choose from a variety of datasets available."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Step 1.")," Visit ",(0,n.yg)("a",{parentName:"p",href:"https://universe.roboflow.com/lakshantha-dissanayake/apple-detection-5z37o/dataset/1"},"this URL")," to access an Apple Detection dataset available publicly on Roboflow Universe")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Step 2.")," Click ",(0,n.yg)("strong",{parentName:"p"},"Create Account")," to create a Roboflow account"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/53.png"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 3.")," Click ",(0,n.yg)("strong",{parentName:"li"},"Download"),", select ",(0,n.yg)("strong",{parentName:"li"},"YOLO v5 PyTorch")," as the ",(0,n.yg)("strong",{parentName:"li"},"Format"),", click ",(0,n.yg)("strong",{parentName:"li"},"show download code")," and click ",(0,n.yg)("strong",{parentName:"li"},"Continue"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/51.png"})),(0,n.yg)("p",null,"This will generate a code snippet that we will use later inside Google Colab training. So please keep this window open in the background."),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:700,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/52.png"})),(0,n.yg)("h3",{id:"train-using-yolov5-on-google-colab"},"Train using YOLOv5 on Google Colab"),(0,n.yg)("p",null,"After we have chosen a public dataset, we need to train the dataset. Here we use a Google Colaboratory environment to perform training on the cloud. Furthermore, we use Roboflow api within Colab to easily download our dataset."),(0,n.yg)("p",null,"Click ",(0,n.yg)("a",{parentName:"p",href:"https://colab.research.google.com/gist/lakshanthad/b47a1d1a9b4fac43449948524de7d374/yolov5-training-for-sensecap-a1101.ipynb"},"here")," to open an already prepared Google Colab workspace, go through the steps mentioned in the workspace and run the code cells one by one."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Note:")," On Google Colab, in the code cell under ",(0,n.yg)("strong",{parentName:"p"},"Step 4"),", you can directly copy the code snippet from Roboflow as mentioned above"),(0,n.yg)("p",null,"It will walkthrough the following:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},"Setup an environment for training"),(0,n.yg)("li",{parentName:"ul"},"Download a dataset"),(0,n.yg)("li",{parentName:"ul"},"Perform the training"),(0,n.yg)("li",{parentName:"ul"},"Download the trained model")),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/18.png"})),(0,n.yg)("p",null,"For an apple detection dataset with 699 images, it took around 7 minutes to finish the training process on Google Colab running on NVIDIA Tesla T4 GPU with 16GB GPU memory."),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/43.png"})),(0,n.yg)("p",null,"If you followed the above Colab project, you know that you can load 4 models to the device all at once. However, please note that only one model can be loaded at a time. This can be specified by the user and will be explained later in this wiki."),(0,n.yg)("h3",{id:"deploy-and-inference"},"Deploy and inference"),(0,n.yg)("p",null,"If you directly want to jump to ",(0,n.yg)("strong",{parentName:"p"},"section 3")," which explains how to deploy the trained AI model into Grove - Vision AI Module and perform inference, ",(0,n.yg)("a",{parentName:"p",href:"#jump3"},"click here"),"."),(0,n.yg)("h2",{id:"2-train-your-own-ai-model-with-your-own-dataset"},(0,n.yg)("span",{id:"jump2"},"2. Train your own AI model with your own dataset")),(0,n.yg)("p",null,"If you want to build specific object detection projects where the public datasets do not have the objects that you want to detect, you might want to build your own dataset.  When you record data for your own dataset, you have to make sure that you cover all angles (360 degrees) of the object, place the object in different environments, different lighting and different weather conditions. After recording your own dataset, you also have to annotate the images in the datset. All these steps will be convered in this section."),(0,n.yg)("p",null,"Eventhough there are different methods of collecting data such as using a mobile phone camera, the best way to collect data is to use the in-built camera on the Grove - Vision AI Module. This is because the colors, image quality and other details will be similar when we perform inference on Grove - Vision AI Module which makes the overall detection more accurate."),(0,n.yg)("h3",{id:"annotate-dataset-using-roboflow"},"Annotate dataset using Roboflow"),(0,n.yg)("p",null,"If you use your own dataset, you will need to annotate all the images in your dataset. Annotating means simply drawing rectangular boxes around each object that we want to detect and assign them labels. We will explain how to do this using Roboflow."),(0,n.yg)("p",null,(0,n.yg)("a",{parentName:"p",href:"https://roboflow.com"},"Roboflow"),' is an annotation tool based online. Here we can directly import the video footage that we have recorded into Roboflow and it will be exported into a series of images. This tool is very convenient because it will let us help distribute the dataset into "training, validation and testing". Also this tool will allow us to add further processing to these images after labelling them. Furthermore, it can easily export the labelled dataset into ',(0,n.yg)("strong",{parentName:"p"},"YOLOV5 PyTorch format")," which is what we exactly need!"),(0,n.yg)("p",null,"For this wiki, we will use a dataset with images containing apples so that we can detect apples later on and do counting as well."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Step 1.")," Click ",(0,n.yg)("a",{parentName:"p",href:"https://app.roboflow.com/login"},"here")," to sign up for a Roboflow account")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Step 2.")," Click ",(0,n.yg)("strong",{parentName:"p"},"Create New Project")," to start our project"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/YOLOV5/2.jpg"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 3.")," Fill in ",(0,n.yg)("strong",{parentName:"li"},"Project Name"),", keep the ",(0,n.yg)("strong",{parentName:"li"},"License (CC BY 4.0)")," and ",(0,n.yg)("strong",{parentName:"li"},"Project type (Object Detection (Bounding Box))"),"  as default. Under ",(0,n.yg)("strong",{parentName:"li"},"What will your model predict?")," column, fill in an annotation group name. For example, in our case we choose ",(0,n.yg)("strong",{parentName:"li"},"apples"),". This name should highlight all of the classes of your dataset. Finally, click ",(0,n.yg)("strong",{parentName:"li"},"Create Public Project"),".")),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:350,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/6.jpg"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 4.")," Drag and drop the images that you have captured using Grove - Vision AI Module")),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/7.png"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 5.")," After the images are processed, click ",(0,n.yg)("strong",{parentName:"li"},"Finish Uploading"),". Wait patiently until the images are uploaded.")),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/4.jpg"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 6.")," After the images are uploaded, click ",(0,n.yg)("strong",{parentName:"li"},"Assign Images"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:300,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/5.jpg"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 7.")," Select an image, draw a rectangular box around an apple, choose the label as ",(0,n.yg)("strong",{parentName:"li"},"apple")," and press ",(0,n.yg)("strong",{parentName:"li"},"ENTER"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/9.png"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 8.")," Repeat the same for the remaining apples")),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/10.png"})),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Note:")," Try to label all the apples that you see inside the image. If only a part of an apple is visible, try to label that too."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 9.")," Continue to annotate all the images in the dataset")),(0,n.yg)("p",null,"Roboflow has a feature called ",(0,n.yg)("strong",{parentName:"p"},"Label Assist")," where it can predict the labels beforehand so that your labelling will be much faster. However, it will not work with all object types, but rather a selected type of objects. To turn this feature on, you simply need to press the ",(0,n.yg)("strong",{parentName:"p"},"Label Assist")," button, ",(0,n.yg)("strong",{parentName:"p"},"select a model"),", ",(0,n.yg)("strong",{parentName:"p"},"select the classes")," and navigate through the images to see the predicted labels with bounding boxes"),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:300,src:"https://files.seeedstudio.com/wiki/YOLOV5/41.png"})),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:400,src:"https://files.seeedstudio.com/wiki/YOLOV5/39.png"})),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:400,src:"https://files.seeedstudio.com/wiki/YOLOV5/40.png"})),(0,n.yg)("p",null,"As you can see above, it can only help to predict annotations for the 80 classes mentioned. If your images do not contain the object classes from above, you cannot use the label assist feature."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 10.")," Once labelling is done, click ",(0,n.yg)("strong",{parentName:"li"},"Add images to Dataset"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/YOLOV5/25.jpg"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 11."),' Next we will split the images between "Train, Valid and Test". Keep the default percentages for the distribution and click ',(0,n.yg)("strong",{parentName:"li"},"Add Images"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:330,src:"https://files.seeedstudio.com/wiki/YOLOV5/26.png"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 12.")," Click ",(0,n.yg)("strong",{parentName:"li"},"Generate New Version"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/YOLOV5/27.jpg"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 13.")," Now you can add ",(0,n.yg)("strong",{parentName:"li"},"Preprocessing")," and ",(0,n.yg)("strong",{parentName:"li"},"Augmentation")," if you prefer. Here we will ",(0,n.yg)("strong",{parentName:"li"},"change")," the ",(0,n.yg)("strong",{parentName:"li"},"Resize")," option to ",(0,n.yg)("strong",{parentName:"li"},"192x192"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/11.png"})),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:450,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/13.png"})),(0,n.yg)("p",null,"Here we change the image size to 192x192 because we will use that size for training and the training will be faster. Otherwise, it will have to convert all images to 192x192 during the training process which consumes more CPU resources and makes the training process slower."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 14.")," Next, proceed with the remaining defaults and click ",(0,n.yg)("strong",{parentName:"li"},"Generate"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/14.png"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 15.")," Click ",(0,n.yg)("strong",{parentName:"li"},"Export"),", select ",(0,n.yg)("strong",{parentName:"li"},"Format")," as ",(0,n.yg)("strong",{parentName:"li"},"YOLO v5 PyTorch"),", select ",(0,n.yg)("strong",{parentName:"li"},"show download code")," and click ",(0,n.yg)("strong",{parentName:"li"},"Continue"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/54.png"})),(0,n.yg)("p",null,"This will generate a code snippet that we will use later inside Google Colab training. So please keep this window open in the background."),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:600,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/55.png"})),(0,n.yg)("h3",{id:"train-using-yolov5-on-google-colab-1"},"Train using YOLOv5 on Google Colab"),(0,n.yg)("p",null,"After we are done with annotating the dataset, we need to train the dataset. Jump to ",(0,n.yg)("a",{parentName:"p",href:"https://wiki.seeedstudio.com/Train-Deploy-AI-Model-Grove-Vision-AI/#train-using-yolov5-on-google-colab"},"this part")," which explains how to train an AI model using YOLOv5 running on Google Colab."),(0,n.yg)("h2",{id:"3-deploy-the-trained-model-and-perform-inference"},(0,n.yg)("span",{id:"jump3"},"3. Deploy the trained model and perform inference")),(0,n.yg)("h3",{id:"grove---vision-ai-module-1"},"Grove - Vision AI Module"),(0,n.yg)("p",null,"Now we will move the ",(0,n.yg)("strong",{parentName:"p"},"model-1.uf2")," that we obtained at the end of the training into Grove - Vision AI Module. Here we will connect the Grove - Vision AI Module with the ",(0,n.yg)("a",{parentName:"p",href:"https://www.seeedstudio.com/Wio-Terminal-p-4509.html"},"Wio Terminal")," to view the inference results."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Note:")," If this is your first time using Arduino, we highly recommend you to refer ",(0,n.yg)("a",{parentName:"p",href:"https://wiki.seeedstudio.com/Getting_Started_with_Arduino"},"Getting Started with Arduino"),". Also, please follow ",(0,n.yg)("a",{parentName:"p",href:"https://wiki.seeedstudio.com/Wio-Terminal-Getting-Started/#getting-started"},"this wiki")," to setup Wio Terminal to work with Arduino IDE."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Step 1.")," Install the latest version of ",(0,n.yg)("a",{parentName:"p",href:"https://www.google.com/chrome"},"Google Chrome")," or ",(0,n.yg)("a",{parentName:"p",href:"https://www.microsoft.com/en-us/edge?r=1"},"Microsoft Edge browser")," and open it")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Step 2.")," Connect Grove - Vision AI Module into your PC via a USB Type-C cable"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:450,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/47.png"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 3.")," Double-click the boot button on Grove - Vision AI Module to enter mass storage mode")),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:220,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/48.png"})),(0,n.yg)("p",null,"After this you will see a new storage drive shown on your file explorer as ",(0,n.yg)("strong",{parentName:"p"},"GROVEAI")),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:280,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/19.jpg"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 4.")," Drag and drop the ",(0,n.yg)("strong",{parentName:"li"},"model-1.uf2")," file to ",(0,n.yg)("strong",{parentName:"li"},"GROVEAI")," drive")),(0,n.yg)("p",null,"As soon as the uf2 finishes copying into the drive, the drive will disappear. This means the uf2 has been successfully uploaded to the module."),(0,n.yg)("p",null,(0,n.yg)("strong",{parentName:"p"},"Note:")," If you have 4 model files ready, you can drag and drop each model one-by-one. Drop first model, wait until it finishes copying, enter boot mode again, drop second model and so on."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 5.")," While the Grove - Vision AI Module is still connected with the PC using USB, connect it to the Wio Terminal via the Grove I2C port as follows")),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:250,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/49.png"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Step 6.")," Install ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/Seeed-Studio/Seeed_Arduino_GroveAI"},"Seeed_Arduino_GroveAI library")," into Arduino IDE and open ",(0,n.yg)("strong",{parentName:"p"},"object_detection.ino")," example")),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"Step 7.")," If you have only loaded one model (with index 1) into Grove - Vision AI Module, it will load that model. However, if you have loaded multiple models, you can ",(0,n.yg)("a",{parentName:"p",href:"https://github.com/Seeed-Studio/Seeed_Arduino_GroveAI/blob/master/examples/object_detection/object_detection.ino#L12"},"specify which model to use")," by changing ",(0,n.yg)("strong",{parentName:"p"},"MODEL",(0,n.yg)("em",{parentName:"strong"},"EXT_INDEX"),"[value]")," where value can take the digits 1,2,3 or 4"))),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-cpp"},"// for example:\nif (ai.begin(ALGO_OBJECT_DETECTION, MODEL_EXT_INDEX_2))\n")),(0,n.yg)("p",null,"The above will load the model with index 2"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 8.")," Since we are detecting apples, we will make a slight change to the code ",(0,n.yg)("a",{parentName:"li",href:"https://github.com/Seeed-Studio/Seeed_Arduino_GroveAI/blob/master/examples/object_detection/object_detection.ino#L55"},"here"))),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-cpp"},'Serial.print("Number of apples: ");\n')),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 9.")," Connect the Wio Terminal to PC, upload this code to the Wio Terminal and open serial monitor of Arduino IDE with 115200 as the baud rate")),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:500,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/42.png"})),(0,n.yg)("p",null,"You will be able to see the detection information on the serial monitor as above."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 10.")," ",(0,n.yg)("a",{parentName:"li",href:"https://files.seeedstudio.com/grove_ai_vision/index.html"},"Click here")," to open a preview window of the camera stream with the detections")),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/31.png"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 11.")," Click ",(0,n.yg)("strong",{parentName:"li"},"Connect")," button. Then you will see a pop up on the browser. Select ",(0,n.yg)("strong",{parentName:"li"},"Grove AI - Paired")," and click ",(0,n.yg)("strong",{parentName:"li"},"Connect"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/32.png"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 12.")," View real-time inference results using the preview window!")),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/33.jpg"})),(0,n.yg)("p",null,'As you can see above, the apples are being detected with bounding boxes around them. Here "0" corresponds to each detection of the same class. If you have multiple classes, they will be named as 0,1,2,3,4 and so on. Also the confidence score for each detected apple (0.8 and 0.84 in above demo) is being displayed!'),(0,n.yg)("h2",{id:"bonus-content"},"Bonus content"),(0,n.yg)("p",null,"If you feel more adventurous, you can continue to follow the rest of the wiki!"),(0,n.yg)("h3",{id:"can-i-train-an-ai-model-on-my-pc"},"Can I train an AI model on my PC?"),(0,n.yg)("p",null,"You can also use your own PC to train an object detection model. However, the training preformance will depend on the hardware you have. You also need to have a PC with a Linux OS for training. We have used an Ubuntu 20.04 PC for this wiki."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 1.")," Clone the ",(0,n.yg)("strong",{parentName:"li"},"yolov5-swift repo")," and install ",(0,n.yg)("strong",{parentName:"li"},"requirements.txt")," in a ",(0,n.yg)("strong",{parentName:"li"},"Python>=3.7.0")," environment")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sh"},"git clone https://github.com/Seeed-Studio/yolov5-swift \ncd yolov5-swift\npip install -r requirements.txt\n")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 2.")," If you followed the steps in this wiki before, you might remember that we exported the dataset after annotating in Robolflow. Also in Roboflow Universe, we downloaded the dataset. In both methods, there was a window like below where it asks what kind of format to download the dataset. So now, please select ",(0,n.yg)("strong",{parentName:"li"},"download zip to computer"),", under ",(0,n.yg)("strong",{parentName:"li"},"Format")," choose ",(0,n.yg)("strong",{parentName:"li"},"YOLO v5 PyTorch")," and click ",(0,n.yg)("strong",{parentName:"li"},"Continue"))),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:400,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/16.png"})),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:400,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/17.png"})),(0,n.yg)("p",null,"After that, a ",(0,n.yg)("strong",{parentName:"p"},".zip file")," will be downloaded to your computer"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 3.")," Copy and paste the .zip file that we downloaded into ",(0,n.yg)("strong",{parentName:"li"},"yolov5-swift")," directory and extract it")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sh"},"# example\ncp ~/Downloads/Apples.v1i.yolov5pytorch.zip ~/yolov5-swift\nunzip Apples.v1i.yolov5pytorch.zip\n")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 4.")," Open ",(0,n.yg)("strong",{parentName:"li"},"data.yaml")," file and edit ",(0,n.yg)("strong",{parentName:"li"},"train")," and ",(0,n.yg)("strong",{parentName:"li"},"val")," directories as follows")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sh"},"train: train/images\nval: valid/images\n")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 5.")," Download a pre-trained model suitable for our training")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sh"},"sudo apt install wget\nwget https://github.com/Seeed-Studio/yolov5-swift/releases/download/v0.1.0-alpha/yolov5n6-xiao.pt\n")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 6.")," Execute the following to start training")),(0,n.yg)("p",null,"Here, we are able to pass a number of arguments:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"img:")," define input image size"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"batch:")," determine batch size"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"epochs:")," define the number of training epochs"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"data:")," set the path to our yaml file"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"cfg:")," specify our model configuration"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"weights:")," specify a custom path to weights"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"name:")," result names"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"nosave:")," only save the final checkpoint"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"cache:")," cache images for faster training")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sh"},"python3 train.py --img 192 --batch 64 --epochs 100 --data data.yaml --cfg yolov5n6-xiao.yaml --weights yolov5n6-xiao.pt --name yolov5n6_results --cache\n")),(0,n.yg)("p",null,"For an apple detection dataset with 987 images, it took around 30 minutes to finish the training process on a Local PC running on NVIDIA GeForce GTX 1660 Super GPU with 6GB GPU memory."),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:1e3,src:"https://files.seeedstudio.com/wiki/SenseCAP-A1101/44.png"})),(0,n.yg)("p",null,"If you followed the above Colab project, you know that you can load 4 models to the device all at once. However, please not that only one model can be loaded at a time. This can be specified by the user and will be explained later in this wiki."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 7.")," If you navigate to ",(0,n.yg)("inlineCode",{parentName:"li"},"runs/train/exp/weights"),", you will see a file called ",(0,n.yg)("strong",{parentName:"li"},"best.pt"),". This is the generated model from training.")),(0,n.yg)("div",{align:"center"},(0,n.yg)("img",{width:600,src:"https://files.seeedstudio.com/wiki/YOLOV5/33.jpg"})),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 8.")," Export the trained model to TensorFlow Lite")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sh"},"python3 export.py --data {dataset.location}/data.yaml --weights runs/train/yolov5n6_results/weights/best.pt --imgsz 192 --int8 --include tflite  \n")),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 9.")," Convert TensorFlow Lite to a UF2 file")),(0,n.yg)("p",null,"UF2 is a file format, developed by Microsoft. Seeed uses this format to convert .tflite to .uf2, allowing tflite files to be stored on the AIoT devices launched by Seeed. Currently Seeed's devices support up to 4 models, each model (.tflite) is less than 1M ."),(0,n.yg)("p",null,"You can specify the model to be placed in the corresponding index with -t."),(0,n.yg)("p",null,"For example:"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"-t 1"),": index 1"),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("inlineCode",{parentName:"li"},"-t 2"),": index 2")),(0,n.yg)("pre",null,(0,n.yg)("code",{parentName:"pre",className:"language-sh"},"# Place the model to index 1\npython3 uf2conv.py -f GROVEAI -t 1 -c runs//train/yolov5n6_results//weights/best-int8.tflite -o model-1.uf2\n")),(0,n.yg)("p",null,"Eventhough you can load 4 models to the device all at once, please not that only one model can be loaded at a time. This can be specified by the user and will be explained later in this wiki."),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("strong",{parentName:"li"},"Step 10.")," Now a file named ",(0,n.yg)("strong",{parentName:"li"},"model-1.uf2")," will be generated. This is the file that we will load into the Grove - Vision AI Module to perform the inference!")),(0,n.yg)("h2",{id:"resources"},"Resources"),(0,n.yg)("ul",null,(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"[Web Page]")," ",(0,n.yg)("a",{parentName:"p",href:"https://docs.ultralytics.com"},"YOLOv5 Documentation"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"[Web Page]")," ",(0,n.yg)("a",{parentName:"p",href:"https://ultralytics.com/hub"},"Ultralytics HUB"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"[Web Page]")," ",(0,n.yg)("a",{parentName:"p",href:"https://docs.roboflow.com"},"Roboflow Documentation"))),(0,n.yg)("li",{parentName:"ul"},(0,n.yg)("p",{parentName:"li"},(0,n.yg)("strong",{parentName:"p"},"[Web Page]")," ",(0,n.yg)("a",{parentName:"p",href:"https://www.tensorflow.org/lite/guide"},"TensorFlow Lite Documentation")))),(0,n.yg)("h2",{id:"tech-support--product-discussion"},"Tech Support & Product Discussion"),(0,n.yg)("p",null,"Thank you for choosing our products! We are here to provide you with different support to ensure that your experience with our products is as smooth as possible. We offer several communication channels to cater to different preferences and needs."),(0,n.yg)("div",{class:"button_tech_support_container"},(0,n.yg)("a",{href:"https://forum.seeedstudio.com/",class:"button_forum"}),(0,n.yg)("a",{href:"https://www.seeedstudio.com/contacts",class:"button_email"})),(0,n.yg)("div",{class:"button_tech_support_container"},(0,n.yg)("a",{href:"https://discord.gg/eWkprNDMU7",class:"button_discord"}),(0,n.yg)("a",{href:"https://github.com/Seeed-Studio/wiki-documents/discussions/69",class:"button_discussion"})))}u.isMDXComponent=!0}}]);