---
description: å¦‚ä½•åœ¨reComputerä¸Šè¿è¡Œæœ¬åœ°LLMæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ 
title: å¦‚ä½•åœ¨reComputerä¸Šè¿è¡Œæœ¬åœ°LLMæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ 
keywords:
- Contributor
image: https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png
slug: /cn/How_to_run_local_llm_text_to_image_on_reComputer
last_update:
  date: 04/01/2024
  author: Bruno
---


# å¦‚ä½•åœ¨reComputerä¸Šè¿è¡Œæœ¬åœ°LLMæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ 

## ä»‹ç»
æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ˜¯ä¸€ç§æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆå›¾åƒçš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹æ¥å—æ–‡æœ¬è¾“å…¥ï¼Œå¦‚æè¿°åœºæ™¯çš„å¥å­æˆ–æ®µè½ï¼Œå¹¶æ ¹æ®æè¿°ç”Ÿæˆå›¾åƒã€‚

è¿™äº›æ¨¡å‹åœ¨åŒ…å«æˆå¯¹æ–‡æœ¬æè¿°å’Œç›¸åº”å›¾åƒçš„å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ ç†è§£æ–‡æœ¬ä¿¡æ¯å’Œè§†è§‰ä¿¡æ¯ä¹‹é—´çš„å…³ç³»ã€‚

è¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç”Ÿæˆä¸æ–‡æœ¬æè¿°ç²¾ç¡®åŒ¹é…çš„é«˜è´¨é‡ã€å¤šæ ·åŒ–å›¾åƒä»æ˜¯äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­çš„ä¸€é¡¹æŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚

## æ•…éšœæ’é™¤

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨éƒ¨ç½²å’Œè¿è¡Œæœ¬åœ° LLM æ–‡æœ¬åˆ°å›¾åƒçš„å‡ ç§æ–¹æ³•ï¼š
1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆåŒ…æ‹¬ TensorFlow å’Œ PyTorchï¼‰
- 1.1. ä½¿ç”¨ Keras Stable Diffusion åˆ›å»ºä¸€ä¸ªç¤ºä¾‹
- 1.2. ä½¿ç”¨ Hugging Face æä¾›çš„æ¨¡å‹ä¹‹ä¸€åˆ›å»ºä¸€ä¸ªç¤ºä¾‹
- 1.3. åˆ›å»ºä¸€ä¸ªå°å‹ Python APIï¼Œé€šè¿‡è°ƒç”¨ Keras å’Œ Hugging Face çš„ API æ¥ç”Ÿæˆå›¾åƒã€‚
3. ä½¿ç”¨ Nvidia å®¹å™¨ã€‚

### ç–‘éš¾è§£ç­”
åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥é‡‡å–ä»¥ä¸‹æ­¥éª¤æ¥å¢åŠ å¯ç”¨å†…å­˜.

1. ç¦ç”¨æ¡Œé¢å›¾å½¢ç”¨æˆ·ç•Œé¢ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ SSH ä½¿ç”¨ Jetsonã€‚è¿™æ ·å¯ä»¥èŠ‚çœçº¦ 800MB å†…å­˜.

2. ç¦ç”¨ ZRAM å¹¶ä½¿ç”¨ Swap. 

ä½ å¯ä»¥åœ¨[è‹±ä¼Ÿè¾¾ Jetson äººå·¥æ™ºèƒ½å®éªŒå®¤](https://www.jetson-ai-lab.com/tips_ram-optimization.html) ä¸­æ‰¾åˆ°è¿™äº›æç¤ºä»¥åŠå¦‚ä½•å®ç°å®ƒä»¬ã€‚ 

## è¦æ±‚

åœ¨æœ¬æ•™ç¨‹ä¸­, æˆ‘ä»¬éœ€è¦ä¸€ä¸ªNvidia [Jetson Orin NX 16GB](https://www.seeedstudio.com/reComputer-J4012-p-5586.html).

<div align="center">
    <img width={800} 
     src="https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_J4012.png" />
</div>

<div class="get_one_now_container" style={{textAlign: 'center'}}>
    <a class="get_one_now_item" href="https://www.seeedstudio.com/reComputer-J4012-p-5586.html?queryID=3d7dba9378be2accafeaff54420edb6a&objectID=5586&indexName=bazaar_retailer_products"><strong><span><font color={'FFFFFF'} size={"4"}> ç«‹å³è´­ä¹° ğŸ–±ï¸</font></span></strong>
    </a>
</div>

æˆ‘ä»¬è¿˜éœ€è¦ç¡®ä¿å®‰è£…äº† **TensorFlow**å’Œ **PyTorch**ï¼Œä¸è¿‡æˆ‘ä¼šåœ¨è¿™é‡ŒæŒ¨ä¸ªè¯´æ˜. 

### æ­¥éª¤1 - åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

Keraså¯ä»¥ä½¿ç”¨TensorFlowæˆ–PyTorchä½œä¸ºåç«¯ã€‚ Hugging Faceä¸»è¦ä½¿ç”¨PyTorch.

è®©æˆ‘ä»¬å®‰è£…TensorFlowå’ŒPyTorch. 

å…³äºå¦‚ä½•ä¸ºJetson Orin NXå®‰è£…TensorFlowå’ŒPyTorchçš„è¯´æ˜åœ¨[Nvidia ç½‘ç«™](https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html).

æˆ‘ä»¬å¯ä»¥å…¨å±€å®‰è£… TensorFlow å’Œ PyTorchï¼Œæˆ–è€…åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£…ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒã€‚

é€šè¿‡ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒï¼Œæˆ‘ä»¬ä¸ä¼šé¢ä¸´é¡¹ç›®æˆ–è½¯ä»¶åŒ…ç‰ˆæœ¬æ··åˆçš„é£é™©. 

è¿™æ˜¯æœ€å¥½çš„æ–¹æ³•ï¼Œå°½ç®¡ Nvidia ç½‘ç«™æ›´å€¾å‘äºä½¿ç”¨å…¨å±€æ–¹æ³•. 

##### TensorFlow 

åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæˆ‘ä½¿ç”¨ kerasStableEnvironment ä½œä¸ºåç§°ï¼Œå› ä¸ºæˆ‘å°†åœ¨ keras ç¤ºä¾‹ä¸­ä½¿ç”¨å®ƒã€‚å¦‚æœä½ æ„¿æ„ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–åç§°ï¼‰
```bash
sudo apt install python3.8-venv
python -m venv kerasStableEnvironment
```
åˆ›å»ºåï¼Œæ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
```bash
source kerasStableEnvironment/bin/activate
```
å½“ç¯å¢ƒæ¿€æ´»æ—¶ï¼Œä½ ä¼šåœ¨æç¤ºç¬¦å‰çœ‹åˆ°å®ƒçš„åç§°
<div align="center"><img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/1_prompt_bash.png" /></div>

è¿›å…¥è™šæ‹Ÿç¯å¢ƒ
```bash
cd kerasStableEnvironment
```
å‡çº§ PIP å¹¶å®‰è£…ä¸€äº›ä¾èµ–é¡¹
```bash
pip install -U pip
pip install -U numpy grpcio absl-py py-cpuinfo psutil portpicker six mock requests gast h5py astor termcolor protobuf keras-applications keras-preprocessing wrapt google-pasta setuptools testresources
```
ä¸º Jetpack 5.1.1 å®‰è£… TensorFlow

è¦æŸ¥æ‰¾æˆ‘ä»¬æ‹¥æœ‰çš„ JetPack ç‰ˆæœ¬ï¼Œè¯·å‘å‡ºä»¥ä¸‹å‘½ä»¤ï¼š
```bash
dpkg -l | grep -i jetpack
```
ç»“æœåº”æ˜¾ç¤º jetpack ç‰ˆæœ¬ï¼š
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/2_jetpack_version.png" />
</div>

```bash
pip install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v511 tensorflow==2.12.0+nv23.05
```
å¦‚æœä½ æœ‰å…¶ä»– JetPack ç‰ˆæœ¬ï¼Œè¯·æ£€æŸ¥ [Nvidiaå®˜ç½‘](https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html)ä½¿ç”¨æ­£ç¡®çš„ URLã€‚. 

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ TensorFlow çš„å®‰è£…
```bash
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
```
è¿™åº”è¯¥è¿”å›ä»¥ä¸‹è¡Œï¼š
```bash
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```

##### PyTorch

è®©æˆ‘ä»¬å®‰è£…ä¸€äº›ä¾èµ–é¡¹
```bash
sudo apt install libopenblas-dev
```
ç°åœ¨ï¼Œä¸ºJetPack 5.1.1å®‰è£…PyTorch
```bash
pip install --no-cache https://developer.download.nvidia.com/compute/redist/jp/v511/pytorch/torch-2.0.0+nv23.05-cp38-cp38-linux_aarch64.whl
```
æ£€æŸ¥å®‰è£…ä»¥åŠCUDAæ˜¯å¦å¯ç”¨
```bash
python -c "import torch; print(torch.cuda.is_available())"
```
å®ƒåº”è¯¥è¿”å› **çœŸ**

ç°åœ¨æˆ‘ä»¬å·²ç»å®‰è£…äº†TensorFlowå’ŒPyTorchï¼Œè®©æˆ‘ä»¬å®‰è£…Keraså¹¶åˆ›å»ºä¸€ä¸ªå›¾åƒ

#### 1.1 Keras

å®‰è£…å **PyTorch** å’Œ **Tensorflow**ï¼Œæˆ‘ä»¬ç°åœ¨å‡†å¤‡å¼€å§‹ä»æ–‡æœ¬æç¤ºåˆ›å»ºå›¾åƒã€‚

ç¡®ä¿ä½ ä»ç„¶åœ¨è™šæ‹Ÿç¯å¢ƒä¸­.

[KerasCV](https://keras.io/keras_cv/) æœ‰ä¸€ä¸ªå®ç°ï¼ˆä»¥åŠå…¶ä»–å‡ ä¸ªï¼‰ [Stability.ai](https://stability.ai/) æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹, [ç¨³å®šæ‰©æ•£](https://github.com/CompVis/stable-diffusion). 

[é€šè¿‡ä½¿ç”¨ KerasCV å®ç°](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/), æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ä¸€äº›æ€§èƒ½ä¼˜åŠ¿ï¼Œæ¯”å¦‚XLAç¼–è¯‘å’Œæ··åˆç²¾åº¦æ”¯æŒã€‚

[ä½ å¯ä»¥åœ¨Kerasç½‘ç«™ä¸Šé˜…è¯»æ›´å¤šä¿¡æ¯](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/)

å®‰è£…kerasåŠå…¶ä¾èµ–é¡¹ã€‚ æˆ‘ä»¬é€‰æ‹©è¿™ä¸ªç‰ˆæœ¬æ˜¯å› ä¸ºå®ƒä»¬ä¸æˆ‘ä»¬å·²å®‰è£…çš„TensorFlowï¼ˆæˆ–PyTorchï¼‰ç‰ˆæœ¬å…¼å®¹. 

```bash
pip install keras-cv==0.5.1
pip install keras==2.12.0
pip install Pillow
```
æ‰“å¼€ä½ å–œæ¬¢çš„ç¼–è¾‘å™¨ï¼Œè¾“å…¥ä»¥ä¸‹ç¤ºä¾‹

```bash
vi generate_image.py
```
```python
import keras_cv
import keras
from PIL import Image

keras.mixed_precision.set_global_policy("mixed_float16")

model = keras_cv.models.StableDiffusion (
        img_width=512,  # we can choose another size, but has to be a mutiple of 128
        img_height=512, # the same above
        jit_compile=True
)

prompt = "a cute magical flying dog, fantasy art, golden color, high quality, highly detailed, elegant, sharp focus, concept art, character concepts, digital painting, mystery, adventure"

image = model.text_to_image (prompt,
        num_steps = 25, #image quality
        batch_size = 1 # how many images to generate at once
)

Image.fromarray(image[0]).save("keras_generate_image.png")
```
åœ¨è¿è¡Œè„šæœ¬æ—¶ï¼Œè¿™é‡Œæœ‰ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/3_statistics.png" />
</div>

è¿‡äº†ä¸€æ®µæ—¶é—´åï¼Œè¿™é‡Œæ˜¯ç»“æœ
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/4_keras_generate_image.png" />
</div>

### æ­¥éª¤ 1.2 - Hugging Face
[Hugging Face](https://huggingface.co/) å°±åƒæ˜¯æœºå™¨å­¦ä¹ çš„ Githubã€‚å®ƒå…è®¸å¼€å‘äººå‘˜æ„å»ºã€éƒ¨ç½²ã€å…±äº«å’Œè®­ç»ƒä»–ä»¬çš„ ML æ¨¡å‹

Hugging Face è¿˜å› å…¶ Transformers Python åº“è€Œé—»åï¼Œè¯¥åº“å¯ç®€åŒ–ä¸‹è½½å’Œè®­ç»ƒ ML æ¨¡å‹çš„è¿‡ç¨‹ã€‚

è®©æˆ‘ä»¬ä½¿ç”¨ä¸€äº›å¯ç”¨çš„æ¨¡å‹ã€‚
å‰å¾€Hugging Faceå¹¶é€‰æ‹©æŸ¥çœ‹æ¨¡å‹ã€‚

åœ¨å·¦ä¾§ï¼Œä½ æœ‰è¿‡æ»¤å™¨ï¼Œå¯ä»¥é€‰æ‹©æˆ‘ä»¬æƒ³è¦æŸ¥çœ‹çš„æ¨¡å‹ç±»å‹ã€‚
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/5_huggingface.png" />
</div>
æœ‰å¾ˆå¤šå¯ç”¨çš„æ¨¡å‹ï¼Œä½†æˆ‘ä»¬åªè®¨è®ºæ–‡æœ¬å¦‚ä½•ç”Ÿæˆå›¾åƒæ¨¡å‹ã€‚

#### è™šæ‹Ÿç¯å¢ƒ
åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒï¼Œå°±åƒæˆ‘ä»¬ä¸Šé¢æ‰€åšçš„é‚£æ ·ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨Hugging Faceï¼Œè€Œä¸ä¼šæ··æ·†åŒ…ç‰ˆæœ¬æˆ–å®‰è£…ä¸éœ€è¦çš„åŒ…. 

```bash
python -m venv huggingfaceTesting
source huggingfaceTesting/bin/activate
```
åˆ›å»ºè™šæ‹Ÿç¯å¢ƒåï¼Œè®©æˆ‘ä»¬è¿›å…¥å®ƒ.
ä½¿ç”¨ä¸Šé¢çš„è¯´æ˜å®‰è£…PyTorch.

```bash
cd huggingfaceTesting
```
####  æ¨¡ç»„
Hugging Face æœ‰å¾ˆå¤š [æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹](https://huggingface.co/models?pipeline_tag=text-to-image&sort=trending). è™½ç„¶ç†è®ºä¸Šå®ƒä»¬åº”è¯¥ä¸æˆ‘ä»¬çš„ Jetson ä¸€èµ·å·¥ä½œï¼Œä½†å¹¶æ²¡æœ‰. 

**stable-diffusion-v1-5**

æˆ‘å°†æµ‹è¯•Runawayçš„[stable-diffusion-v1-5ç‰ˆæœ¬](https://huggingface.co/runwayml/stable-diffusion-v1-5). 

åœ¨æ¨¡å‹å¡ä¸Šï¼Œä»–ä»¬æœ‰ä½¿ç”¨æ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ . 
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/6_stable_diffusion_v1_5.png"/>
</div>

æˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face æ‰©æ•£å™¨åº“ã€‚
åœ¨è™šæ‹Ÿç¯å¢ƒä¸­ï¼ˆæ¿€æ´»åï¼‰å®‰è£…ä¾èµ–é¡¹ã€‚
```bash
pip install diffusers transformers accelerate
```
ç°åœ¨æˆ‘ä»¬å·²ç»å®‰è£…äº†æ‰€æœ‰ä¾èµ–é¡¹ï¼Œè®©æˆ‘ä»¬æ¥è¯•è¯•æ¨¡å‹ã€‚
ä½¿ç”¨æ‚¨å–œæ¬¢çš„ç¼–è¾‘å™¨ï¼Œå¤åˆ¶ä»¥ä¸‹ä»£ç ï¼ˆä¹Ÿå¯åœ¨æ¨¡å‹å¡é¡µé¢ä¸­æ‰¾åˆ°ï¼‰ï¼š
```python
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe = pipe.to("cuda")

prompt = "a master jedi cat in star wars holding a lightsaber, wearing a jedi cloak hood, dramatic, cinematic lighting"
image = pipe(prompt).images[0]  
    
image.save("cat_jedi.png")

```
è®©æˆ‘ä»¬è¯•è¯•è¿™ä¸ªæ¨¡å‹. 
```bash
python stableDiffusion.py
```
**æç¤ºï¼š** è¿™å°†å ç”¨å¤§é‡ç©ºé—´ã€‚æ­£åœ¨ä¸‹è½½æ¨¡å‹çš„æ£€æŸ¥ç‚¹ã€‚åªèƒ½ä¸‹è½½ä¸€æ¬¡ã€‚

<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/7_model_download.png"/>
</div>
è¿‡äº†ä¸€æ®µæ—¶é—´ï¼Œç»“æœæ˜¯è¿™æ ·çš„
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/8_result_stablediffusion.png"/>
</div>

**SDXL-Turbo**

Here's another model we can try. [SDXL Turbo from Stability AI.](https://huggingface.co/stabilityai/sdxl-turbo)
Copy the following code
```python
from diffusers import AutoPipelineForText2Image
import torch

pipe = AutoPipelineForText2Image.from_pretrained("stabilityai/sdxl-turbo", torch_dtype=torch.float16, variant="fp16")
pipe.to("cuda")

prompt = "full body, cat dressed as a Viking, with weapon in his paws, battle coloring, glow hyper-detail, hyper-realism, cinematic"

image = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]
image.save("sdxl-turbo.png")
```
[This prompt was taken from a Medium article written by Daria Wind](https://medium.com/phygital/top-40-useful-prompts-for-stable-diffusion-xl-008c03dd0557)

This one is really fast generating an image. Takes almost 30s, from running the script until it exits. 
Here's the result
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/9_sdxl-turbo.png"/>
</div>

We can also try other models, like models trained specifically for anime or cyberpunk. 

There will be some models that will not work. It can be because of several factors - memory, available CPUs or even Swap memory. 

### Step 1.3 - Create a small API
Let's now crete a small API with Flask to use to generate an image given a prompt and return it to the caller. 

Imagine that you have the Jetson running and want to be able to generate an image by calling a API - your personal LLM image-to-text.

There are already projects that do this (like the one we're going to see later), but nothing beats doing it yourself. 

Let's create a new Virtual Environment
```bash
python -m venv imageAPIGenerator
```
Activate the environment and enter it
```bash
source  imageAPIGenerator/bin/activate
cd imageAPIGenerator
```
We're going go use Flask for this. [FlasK](https://flask.palletsprojects.com/en/3.0.x/) is web application framework written in Python. It's small enough for our purpose. 

Install Flask.
```bash
pip install Flask
```
After installing it, let's install all the other dependencies that we need. Just for demonstration purposes, we're going to use Keras, because it has the least dependencies. 

Install TensorFlow. Follow the instructions above. 
Next, install Keras.
```bash
pip install keras-cv==0.5.1
pip install keras==2.12.0
pip install Pillow
```
Now, let's start to write our application. 

```bash
vi app.py
```
For those who don't know what Flask is or does, let's try a small example. 

```python
from flask import Flask

app = Flask (__name__)


@app.route("/generate_image")
def generate_image_api():
    return "<h2>Hello World !</h2>"


if __name__ == "__main__":
    app.run(host='',port=8080)
```
To run, execute the python script:
```bash
python app.py
```
You should see the following:
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/10_run_flask.png"/>
</div>

Now, open a browser and try to access your Jetson device with the 8080 port.
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/11_browser_access.png"/>
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/12_accessed_flask.png"/>
</div>

What we did was importing the Flask class
```python
import Flask
```
We next created an instance of the Flask class
```python
app = Flask(__name__)
```
We next create a route decorator to tell Flask what URL will trigger our function
 ```python
@app.route("/generate_image")
```
When using generate_image in the URL, we will trigger our function
```python
def generate_image_api():
    return "<h2>Hello World !</h2>"
```
We can also use curl to access our API
```bash
curl http://192.168.2.230:8080/generate_image
```
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/13_curl.png"/>
</div>
Now that we know how to create a API, let's dive into it and write it.

```bash
vi app.py
```
And paste the code
```python
from flask import Flask, request, send_file
import random, string
import keras_cv
import keras
from PIL import Image

#define APP
app = Flask (__name__)


#option for keras
keras.mixed_precision.set_global_policy("mixed_float16")

# generate custom filename
def generate_random_string(size):
    """Generate a random string of specified size."""
    return ''.join(random.choices(string.ascii_letters + string.digits, k=size))


"""
    This is the function that will generate the image
    and save it using a random created filename
"""
def generate_image(prompt):

    model = keras_cv.models.StableDiffusion (
        img_width=512,  # we can choose another size, but has to be a mutiple of 128
        img_height=512, # the same above
        jit_compile=True
    )

    image = model.text_to_image (prompt,
            num_steps = 25,
            batch_size = 1
    )

    # image filename
    filename = generate_random_string(10) + ".png"
    Image.fromarray(image[0]).save(filename)
    return filename # return filename to send it to client


#define routes
# Use this to get the prompt. we're going to receive it using GET
@app.route("/generate_image", methods=["GET"])
def generate_image_api():
    # get the prompt
    prompt = request.args.get("prompt")
    if not prompt:
        # let's define a default prompt
        prompt = "A cinematic shot of a baby racoon wearing an intricate italian priest robe."

    image_name = generate_image(prompt)
    return send_file(image_name, mimetype='image/png')


if __name__ == "__main__":
    app.run(host='0.0.0.0',port=8080)
```
**REMEMBER:** This is not code ready for the Internet. We don't have any security measures whatsoever. 

Let's run it.

In a browser, type in the URL *http://jetsonIP:8080/generate_image* and wait. 

If we don't give it a prompt, it will use the default one we've set.

In the CLI, you can see the image being generated
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/14_generating_image_api.png"/>
</div>

And in the browser, after a while, we can see the image
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/15_image_API_generated.png"/>
</div>

We can also see the image has been sent
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/16_cli_generated.png"/>
</div>

We can also use curl to get the image and save it.
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/17_cli_generating.png"/>
</div>

If we want to give it a prompt (as we should), the URL will look like
*http://jetsonIP:8080/generate_image?prompt=<your_prompt>*

We can expand this example to build a better page, like having some text boxes for user input, a nice background, etc. But this is for another project. 

### Step 2 - Nvidia LLM
#### Stable Diffusion v1.5
We can use the [Jetson Containers](https://github.com/dusty-nv/jetson-containers) project to run [stable-diffusion-webui using AUTOMATIC1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui). 
Jetson Containers project is run by [Dusty Franklin](https://github.com/dusty-nv), a NVIDIA employee.

NVIDIA has the [NVIDIA Jetson Generative AI Lab](https://www.jetson-ai-lab.com/tutorial-intro.html) project that has a lot of tutorials on Machine Learning. 

We're going to use [Stable Diffusion tutorial](https://www.jetson-ai-lab.com/tutorial_stable-diffusion.html) for this. 

Let's clone the github repository, enter the repository and install dependencies
```bash
git clone https://github.com/dusty-nv/jetson-containers
cd jetson-containers/
sudo apt update; sudo apt install -y python3-pip
pip3 install -r requirements.txt
```

Now that we have everything we need, let's run the container with the *stable-diffusion-webui autotag*

```bash
./run.sh $(./autotag stable-diffusion-webui)
```
It will start to run the container. 

After a while, it will say that there's a compatible container and if we want to proceed. 
```bash
Found compatible container dustynv/stable-diffusion-webui:r35.3.1 (2024-02-02, 7.3GB) - would you like to pull it? [Y/n] 
```
It will start downloading the container. 
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/18_container_downloading.png"/>
</div>

After finishing, it will download the model and run the server on port 7860.

Here for me it didn't work at first. No checkpoint would appear to choose from, no matter how many times I would press the refresh 
button. 
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/20_no_checkpoint.png"/>
</div>

I found out that I had 100% space occupied. 
```bash
feiticeir0@JetsonOrin:~$ df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/nvme0n1p1   79G   79G     0 100% /
none            7,4G     0  7,4G   0% /dev
tmpfs           7,6G     0  7,6G   0% /dev/shm
tmpfs           1,6G   19M  1,5G   2% /run
tmpfs           5,0M  4,0K  5,0M   1% /run/lock
tmpfs           7,6G     0  7,6G   0% /sys/fs/cgroup
/dev/loop0      162M  162M     0 100% /snap/chromium/2797
/dev/loop2      128K  128K     0 100% /snap/bare/5
/dev/loop1       70M   70M     0 100% /snap/core22/1125
/dev/loop3       65M   65M     0 100% /snap/cups/1025
/dev/loop4       92M   92M     0 100% /snap/gtk-common-themes/1535
/dev/loop6      162M  162M     0 100% /snap/chromium/2807
/dev/loop5      483M  483M     0 100% /snap/gnome-42-2204/174
/dev/loop7       35M   35M     0 100% /snap/snapd/21185
tmpfs           1,6G  4,0K  1,6G   1% /run/user/1000
```
I've been testing other models and they occupied all the space. 
If this happens to you, just go to your home directory, to the hidden cache directory and delete the huggingface directory. 
```bash
cd ~/.cache
rm -rf huggingface
```
Now you should have space available. Or just get a new drive, with more space. :)

Now the model is being downloaded.
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/21_mode_downloading.png"/>
</div>
And we have a checkpoint
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/22_checkpoint.png"/>
</div>

Open your browser and head to your Jetson IP address and port to run the AUTOMATIC1111's Stable Diffusion webgui

*http://JetsonIPAddress:7860*
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/19_jetson_webgui.png"/>
</div>

Now we can play with it. 
Here's some images created with the default model. 
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/23_creating_image1.gif"/>
</div>
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/24_creating_image2.gif"/>
</div>

#### Stable Diffusion XL
AUTOMATIC1111 supports other models. Let's try with Stable Diffusion XL. It has 6.6 billion parameters. 

To add another model, and to be easier to download it, let's define some variables, change permissions and download the models. 
This is an example from [NVIDIA's Tutorial](https://www.jetson-ai-lab.com/tutorial_stable-diffusion-xl.html).

```bash
CONTAINERS_DIR=<where_jetson-containers_is_located>
MODEL_DIR=$CONTAINERS_DIR/data/models/stable-diffusion/models/Stable-diffusion/
sudo chown -R $USER $MODEL_DIR
```
Now, download the model
```bash
wget -P $MODEL_DIR https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors
wget -P $MODEL_DIR https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0.safetensors
```
With the models downloaded, let's refresh the checkpoints drop-down if you have the container running, or launch the container again. 

Now we have two more models available to us. 
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/25_models.png"/>
</div>

This is an example generated with the XL model, with the following prompt:
>A Portrait, fashionable model wearing futuristic clothing, in a cyberpunk roof-top environment, with a neon-lit city background, backlit by vibrant city glow, fashion photography


<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/26_neon_xl.png"/>
</div>
Try it. Remember that, it may not work with some options selected.

#### Adding other models
We can also add a lot of more models. Besides Hugging Face, [Civitai](https://civitai.com/) is another hub with more models to choose from. Civitai has some NSFW models, so consider yourself warned. 

Select the one you want, download the checkpoints and place them in the models directory
```bash
/home/<user>/<jetson-containers-location>/data/models/stable-diffusion/models/Stable-diffusion/
```
I'm going to download and try a model named [DreamShaper XL](https://civitai.com/models/112902/dreamshaper-xl). 
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/27_dreamshaperxl.png"/>
</div>

Remember that some models may not work. 

You need to play with the settings and read the model card to know what settings may work best (if at all). 

For example, this model card says that the sampling steps should be 4-8, sampling method should be DPM++ SDE Karras, etc... 

Download the model checkpoint and add it to the above directory. 

After refreshing , you should have the model ready to select. 
When selecting, AUTOMATIC1111 will optimize the model. 

If it gets getting killed or an error appears, get more space. It was happening to me and after getting more space, everything worked out. 

Using the following prompt
>holding a staff, orbstaff <lora:orbstaff:0.60> , ,(by Gabriel Isak and Adam Elsheimer:1.20), (by Jon Whitcomb and Bayard Wu and Malcolm Liepke0.80),8k , professional fashion shot

[from this image](https://civitai.com/images/8570722),
without the negative prompt, I got the following result

<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/28_dreamshaperxl_image_result.png"/>
</div>

with these settings:
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/29_dreamshaperXL_settings.png"/>
</div>

Remeber the prompt above for the cyberpunk girl using the *Stable Diffusion XL* model? 

Here's a new image, with the same prompt, generated with *DreamShaper XL* with the same settings above
<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/wiki-ranger/Contributions/Nvidia_Jetson_recomputer_LLM_texto-to-image/30_cyberpunkGirl.png"/>
</div>

As you can see, wonderful images can be created, granting that you know the parameters to tune. :)

I've learned that bigger images tend to produce better results. 

Hope you've learn how to generate images using the Nvidia Jetson NX 16GB and how to use it as a server to generate images on demand. 


## âœ¨ Contributor Project

- This project is supported by the Seeed Studio [Contributor Project](https://github.com/orgs/Seeed-Studio/projects/6/views/1?pane=issue&itemId=56418890).
- Thanks [Bruno's efforts](https://github.com/Seeed-Studio/wiki-documents/issues/1029) and your work will be [exhibited](https://wiki.seeedstudio.com/Honorary-Contributors/).

## Tech Support & Product Discussion

Thank you for choosing our products! We are here to provide you with different support to ensure that your experience with our products is as smooth as possible. We offer several communication channels to cater to different preferences and needs.

<div class="button_tech_support_container">
<a href="https://forum.seeedstudio.com/" class="button_forum"></a> 
<a href="https://www.seeedstudio.com/contacts" class="button_email"></a>
</div>

<div class="button_tech_support_container">
<a href="https://discord.gg/eWkprNDMU7" class="button_discord"></a> 
<a href="https://github.com/Seeed-Studio/wiki-documents/discussions/69" class="button_discussion"></a>
</div>