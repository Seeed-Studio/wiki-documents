<!doctype html>
<html lang="en-US" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Sensor/Grove/Grove_Sensors/AI-powered/Grove-vision-ai-v2/grove_vision_ai_v2_sscma" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">Deploying Models from Datasets to Grove Vision AI V2 | Seeed Studio Wiki</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://wiki.seeedstudio.com/grove_vision_ai_v2_sscma/"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Deploying Models from Datasets to Grove Vision AI V2 | Seeed Studio Wiki"><meta data-rh="true" name="description" content="Guidance on how to go from owned datasets, labelling, training and deployment to Grove Vision AI V2."><meta data-rh="true" property="og:description" content="Guidance on how to go from owned datasets, labelling, training and deployment to Grove Vision AI V2."><meta data-rh="true" name="keywords" content="SSCMA,Vision AI"><meta data-rh="true" property="og:image" content="https://files.seeedstudio.com/wiki/seeed_logo/logo_2023.png"><meta data-rh="true" name="twitter:image" content="https://files.seeedstudio.com/wiki/seeed_logo/logo_2023.png"><link data-rh="true" rel="icon" href="/img/S.png"><link data-rh="true" rel="canonical" href="https://wiki.seeedstudio.com/grove_vision_ai_v2_sscma/"><link data-rh="true" rel="alternate" href="https://wiki.seeedstudio.com/grove_vision_ai_v2_sscma/" hreflang="en-US"><link data-rh="true" rel="alternate" href="https://wiki.seeedstudio.com/grove_vision_ai_v2_sscma/" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Seeed Studio Wiki RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Seeed Studio Wiki Atom Feed">

<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n,g){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var m=t.getElementsByTagName(a)[0],r=t.createElement(a);r.async=!0,r.src="https://www.googletagmanager.com/gtm.js?id=GTM-M4JG2HVB",m.parentNode.insertBefore(r,m)}(window,document,"script","dataLayer")</script>


<link rel="icon" href="/img/S.png">
<link rel="manifest" href="/manifest.json">
<meta name="theme-color" content="rgb(37, 194, 160)">


<link rel="search" type="application/opensearchdescription+xml" title="Seeed Studio Wiki" href="/opensearch.xml">
<script src="https://viewer.altium.com/client/static/js/embed.js" async></script>
<script src="/js/custom.js" async></script><link rel="stylesheet" href="/assets/css/styles.f264e8be.css">
<link rel="preload" href="/assets/js/runtime~main.be8ea93f.js" as="script">
<link rel="preload" href="/assets/js/main.5685c800.js" as="script">
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M4JG2HVB" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:#013949;color:#FFFFFF" role="banner"><div class="content_knG7 announcementBarContent_xLdY">Building Sustainable Growth, Strengthening Local Partnerships. Join   the <a target="_blank" href="https://wiki.seeedstudio.com/ranger/">Seeed Studio Ranger Program</a> now! </div></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Getting_Started/"><div class="navbar__logo"><img src="https://files.seeedstudio.com/wiki/wiki-platform/SeeedStudio.png" alt="Seeed Studio" class="themedImage_ToTc themedImage--light_HNdA navbar_logo_items"><img src="https://files.seeedstudio.com/wiki/wiki-platform/seeed_white_logo.png" alt="Seeed Studio" class="themedImage_ToTc themedImage--dark_i4oU navbar_logo_items"></div></a><div class="navbar__item dropdown dropdown--hoverable"><a class="navbar__link navbar_dorp_items" aria-haspopup="true" aria-expanded="false" role="button" href="/Getting_Started/">Getting Started</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/Sensor_Network/">Sensor and Sensing</a></li><li><a class="dropdown__link" href="/Network/">Networking</a></li><li><a class="dropdown__link" href="/Edge_Computing/">Edge Computing</a></li><li><a class="dropdown__link" href="/Cloud/">Cloud</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a class="navbar__link navbar_dorp_items" aria-haspopup="true" aria-expanded="false" role="button" href="/topicintroduction/">Technology</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/tinyml_topic/">TinyML</a></li><li><a class="dropdown__link" href="/ModelAssistant_Introduce_Overview/">SenseCraft Model Assistant</a></li><li><a class="dropdown__link" href="/home_assistant_topic/">Home Assistant</a></li><li><a class="dropdown__link" href="/open_source_topic/">Open Source</a></li><li><a class="dropdown__link" href="/edge_ai_topic/">Edge AI</a></li><li><a class="dropdown__link" href="/cn/Getting_Started/">ÁüΩÈÄíÁßëÊäÄ Wiki ÊñáÊ°£Âπ≥Âè∞ÔºàÊµãËØïÔºâ</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a class="navbar__link navbar_dorp_items" aria-haspopup="true" aria-expanded="false" role="button" href="/knowledgebase/">FAQs</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/Jetson_FAQ/">NVIDIA Jetson Series</a></li><li><a class="dropdown__link" href="/XIAO_FAQ/">Seeed Studio XIAO Series</a></li><li><a class="dropdown__link" href="/reComputer_R1000_FAQ/">reComputer R1000 Series</a></li><li><a class="dropdown__link" href="/reTerminal-new_FAQ/">reTerminal</a></li><li><a class="dropdown__link" href="/FAQs_For_openWrt/">reRouter</a></li><li><a class="dropdown__link" href="/ODYSSEY_FAQ/">Odyssey</a></li><li><a class="dropdown__link" href="/wio_terminal_faq/">Wio Terminal</a></li><li><hr style="margin: 8px 0;"></li><li><a href="https://discord.com/invite/eWkprNDMU7" target="_blank" rel="noopener noreferrer" class="dropdown__link">Discord<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://www.seeedstudio.com/contacts" target="_blank" rel="noopener noreferrer" class="dropdown__link">Email<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://forum.seeedstudio.com/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Forum<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://github.com/Seeed-Studio/wiki-documents/discussions/69" target="_blank" rel="noopener noreferrer" class="dropdown__link">Have Suggestions?<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a class="navbar__link navbar_dorp_items" aria-haspopup="true" aria-expanded="false" role="button" href="/ranger/">Rangers</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/ranger/">Rangers</a></li><li><a href="https://github.com/orgs/Seeed-Studio/projects/6?pane=issue&amp;itemId=30957479" target="_blank" rel="noopener noreferrer" class="dropdown__link">Contributors(GitHub)<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="navbar__items navbar__items--right"><a href="https://www.seeedstudio.com/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link navbar_doc_right_items">Bazaar üõçÔ∏è</a><a href="https://wiki-gpt.seeedstudio.com/chat" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link navbar_doc_right_items">AI Bot ü§ñÔ∏è</a><a href="https://sensecraft.seeed.cc/ai/#/home" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link navbar_doc_right_items">SenseCraft AI</a><a href="https://sensecraft.seeed.cc/ai/#/home" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-SSCMA"></a><a href="https://github.com/Seeed-Studio/wiki-documents" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sideboard_calss"><a class="menu__link" href="/Getting_Started/">Getting Started</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sideboard_calss"><a class="menu__link" href="/weekly_wiki/">Weekly Wiki</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sideboard_calss"><a class="menu__link" href="/Sensor_Network/">Sensing</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/Grove_System/">Grove</a><button aria-label="Toggle the collapsible sidebar category &#x27;Grove&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/Grove_Sensor_Intro/">Grove Sensor</a><button aria-label="Toggle the collapsible sidebar category &#x27;Grove Sensor&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/Grove_SEN5X_All_in_One/">Multiple in one</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/Grove-Vision-AI-Module/">AI-powered</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/Grove-Vision-AI-Module/">Grove Vision AI</a><button aria-label="Toggle the collapsible sidebar category &#x27;Grove Vision AI&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/grove_vision_ai_v2/">Grove Vision AI V2</a><button aria-label="Toggle the collapsible sidebar category &#x27;Grove Vision AI V2&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/grove_vision_ai_v2_software_support/">Software Support</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/Grove-vision-ai-v2-camera-supported/">External Camera supported</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/grove_vision_ai_v2_sscma/">Deploying Models from Datasets to Grove Vision AI V2</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/grove_vision_ai_v2_himax_sdk/">Development</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-5 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/connect_vision_ai_v2_to_sensecap_mate/">Application</a></div></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/grove_gesture_paj7660/">Grove Smart IR Gesture Sensor (PAJ7660)</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/Grove-AHT20-I2C-Industrial-Grade-Temperature&amp;Humidity-Sensor/">Temp &amp; Humi</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/Grove-1-Wire_Thermocouple_Amplifier-MAX31850K/">Temperature</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/Grove-Capacitive_Moisture_Sensor-Corrosion-Resistant/">Soli Humidity</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/grove_ultrasonic_sensor_sms812/">Proximity</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/Grove-Laser_PM2.5_Sensor-HM3301/">Air Quality</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/Seeed_Gas_Sensor_Selection_Guide/">Gas</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/Sensor_barometer/">Barometer</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/grove-d7s-vibration-sensor/">Accelerometer</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/grove-lightning-sensor-as3935/">Light</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/Sensor_biomedicine/">Biometric</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/Sensor_sound/">Sound</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/Grove-Touch_Sensor/">Touch</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/Sensor_liquid/">Liquid</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/Sensor_motion/">Motion</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/grove_adc_for_load_cell_hx711/">Weight</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/Grove_network_module_intro/">Grove Network Module</a><button aria-label="Toggle the collapsible sidebar category &#x27;Grove Network Module&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/Grove_Accessories_Intro/">Grove Accessories</a><button aria-label="Toggle the collapsible sidebar category &#x27;Grove Accessories&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/SenseCAP_introduction/">SenseCAP</a><button aria-label="Toggle the collapsible sidebar category &#x27;SenseCAP&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Sensor/SenseCAP/SenseCAP_Indicator/Get_started_with_SenseCAP_Indicator/">SenseCAP Indicator</a><button aria-label="Toggle the collapsible sidebar category &#x27;SenseCAP Indicator&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/watcher/">SenseCAP Watcher</a><button aria-label="Toggle the collapsible sidebar category &#x27;SenseCAP Watcher&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/mmwave_radar_Intro/">mmWave Radar Sensor</a><button aria-label="Toggle the collapsible sidebar category &#x27;mmWave Radar Sensor&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/SeeedStudio_XIAO_Series_Introduction/">XIAO</a><button aria-label="Toggle the collapsible sidebar category &#x27;XIAO&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Wio_Terminal_Intro/">Wio Terminal</a><button aria-label="Toggle the collapsible sidebar category &#x27;Wio Terminal&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/reSpeaker_usb_v3/">ReSpeaker Lite</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/Ultra_Sonic_range_measurement_module/">Other Sensing Modules</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/Wio/">Other Microcontrollers</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sideboard_calss"><a class="menu__link" href="/Network/">Network</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/meshtastic_introduction/">Meshtastic Network</a><button aria-label="Toggle the collapsible sidebar category &#x27;Meshtastic Network&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Network/SenseCAP_Network/SenseCAP_Gateway_Intro/">SenseCAP Gateway</a><button aria-label="Toggle the collapsible sidebar category &#x27;SenseCAP Gateway&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/SenseCAP_K1100_Intro/">SenseCAP K1100</a><button aria-label="Toggle the collapsible sidebar category &#x27;SenseCAP K1100&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/M2_Kit_Getting_Started/">SenseCAP LoRaWAN Starter Kit</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/reRouter_Intro/">Raspberry Pi Solutions</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Rockchip_network_solutions/">Rockchip Solutions</a><button aria-label="Toggle the collapsible sidebar category &#x27;Rockchip Solutions&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/The-Things-Indoor-Gateway/">Other Network Devices</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sideboard_calss"><a class="menu__link" href="/Edge_Computing/">Edge Computing</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/raspberry-pi-devices/">Raspberry Pi Devices</a><button aria-label="Toggle the collapsible sidebar category &#x27;Raspberry Pi Devices&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/NVIDIA_Jetson/">NVIDIA Jetson¬Æ</a><button aria-label="Toggle the collapsible sidebar category &#x27;NVIDIA Jetson¬Æ&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/Edgebox-ESP-100-Arduino/">ESP Devices</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/BeagleBone/">BeagleBone¬Æ</a><button aria-label="Toggle the collapsible sidebar category &#x27;BeagleBone¬Æ&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/ODYSSEY_Intro/">ODYSSEY</a><button aria-label="Toggle the collapsible sidebar category &#x27;ODYSSEY&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/reServer-Getting-Started/">Other Edge Devices</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sideboard_calss"><a class="menu__link" href="/Cloud/">Cloud</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/Cloud_Chain/SenseCAP_Dashboard/Dashboard_Basics/">SenseCAP Dashboard</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/Cloud_Chain/SenseCAP_Portal/QuickStart/">SenseCAP Portal</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/SenseCAP_Hotspot_APP/">SenseCAP Hotspot APP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/sensecraft_app/">SenseCraft APP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/How_to_Use_SenseCAP_AI_on_SenseCAP_Portal_and_SenseCAP_Mate_APP/">SenseCAP AI</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/Cloud_Chain/SenseCAP_API/SenseCAP_API_Introduction/">SenseCAP API</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/SenseCraft_AI/">SenseCraft</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sideboard_calss"><a class="menu__link" href="/topicintroduction/">Technology Topics</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/home_assistant_topic/">Home Assistant</a><button aria-label="Toggle the collapsible sidebar category &#x27;Home Assistant&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/tinyml_topic/">TinyML</a><button aria-label="Toggle the collapsible sidebar category &#x27;TinyML&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/open_source_topic/">Open Source</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/edge_ai_topic/">Edge AI</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sideboard_calss"><a class="menu__link" href="/Contributor/">Contributions</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Contribution-Guide/">Github Contributions Guide</a><button aria-label="Toggle the collapsible sidebar category &#x27;Github Contributions Guide&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/community_sourced_projects/">Community Sourced Projects</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/Scale-up-Your-Creation-with-Fusion/">Scale up Your Creation with Seeed Studio Fusion</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sideboard_calss"><a class="menu__link" href="/popularplatforms/">Popular Platforms</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Arduino/">Arduino</a><button aria-label="Toggle the collapsible sidebar category &#x27;Arduino&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/Raspberry_Pi/">Raspberry Pi</a><button aria-label="Toggle the collapsible sidebar category &#x27;Raspberry Pi&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/microbit_wiki_page/">Micro:bit</a><button aria-label="Toggle the collapsible sidebar category &#x27;Micro:bit&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item sideboard_calss"><a class="menu__link" href="/discontinuedproducts/">Discontinued Products</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/ReSpeaker/">Product List</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/About/">About</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/License/">License</a></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_z5aJ"><div class="docItemContainer_c0TR"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/Grove_System/"><span itemprop="name">Grove</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/Grove_Sensor_Intro/"><span itemprop="name">Grove Sensor</span></a><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">AI-powered</span><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/grove_vision_ai_v2/"><span itemprop="name">Grove Vision AI V2</span></a><meta itemprop="position" content="4"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Deploying Models from Datasets to Grove Vision AI V2</span><meta itemprop="position" content="5"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Deploying Models from Datasets to Grove Vision AI V2</h1><p>Welcome to this comprehensive tutorial where we&#x27;ll embark on a journey to turn your dataset into a fully functional model for deployment on the Grove Vision AI V2. In this guide, we&#x27;ll navigate through the initial steps of labeling our dataset with Roboflow&#x27;s intuitive tools, progressing to model training within the collaborative environment of Google Colab.</p><p>We&#x27;ll then move on to deploying our trained model using the SenseCraft Model Assistant, a process that bridges the gap between training and real-world application. By the end of this tutorial, not only will you have a custom model running on Grove Vision AI V2, but you&#x27;ll also be equipped with the knowledge to interpret and utilize the results of your model&#x27;s predictions.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/0.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>From dataset to model landing, we will have the following main steps.</p><ol><li><p><a href="#labelled-datasets">Labelled Datasets</a> ‚Äî‚Äî This chapter focuses on how to obtain datasets that can be trained into models. There are two main ways to do this. The first is to use the labelled datasets provided by the Roboflow community, and the other is to use your own scenario-specific images as datasets, but you need to manually go through the labelling yourself.</p></li><li><p><a href="#training-dataset-exported-model">Training Dataset Exported Model</a> ‚Äî‚Äî This chapter focuses on how to train to get a model that can be deployed to Grove Vision AI V2 based on the dataset obtained in the first step, by using the Google Colab platform.</p></li><li><p><a href="#upload-models-via-sensecraft-model-assistant">Upload models via SenseCraft Model Assistant</a> ‚Äî‚Äî This section describes how to use the exported model file to upload the model to Grove Vision AI V2 using the SenseCraft Model Assistant.</p></li><li><p><a href="#common-protocols-and-applications-of-the-model">Common protocols and applications of the model</a> ‚Äî‚Äî Finally, we will introduce SenseCraft AI&#x27;s unified data communication format so that you can utilise the maximum potential of your devices and models to make applications that fit your scenarios.</p></li></ol><p>So let&#x27;s dive in and begin the exciting process of bringing your data to life.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="materials-required">Materials Required<a href="#materials-required" class="hash-link" aria-label="Direct link to Materials Required" title="Direct link to Materials Required">‚Äã</a></h2><p>Before you start, you may need to prepare the following equipment.</p><div class="table-center"><table align="center"><tr><th>Seeed Studio XIAO ESP32S3</th><th>Grove Vision AI V2</th><th>OV5647-62 FOV Camera Module<br>for Raspberry Pi 3B+4B</th></tr><tr><td><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/SeeedStudio-XIAO-ESP32S3/img/xiaoesp32s3.jpg" style="width:250px;height:auto" class="img_ev3q"></div></td><td><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/grove-vision-ai-v2/14.jpg" style="width:250px;height:auto" class="img_ev3q"></div></td><td><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/grove-vision-ai-v2/11.png" style="width:250px;height:auto" class="img_ev3q"></div></td></tr><tr><td><div class="get_one_now_container" style="text-align:center"><a href="https://www.seeedstudio.com/XIAO-ESP32S3-p-5627.html" target="_blank" rel="noopener noreferrer" class="get_one_now_item"><strong><span><font color="FFFFFF" size="4"> Get One Now üñ±Ô∏è</font></span></strong></a></div></td><td><div class="get_one_now_container" style="text-align:center"><a href="https://www.seeedstudio.com/Grove-Vision-AI-Module-V2-p-5851.html" target="_blank" rel="noopener noreferrer" class="get_one_now_item"><strong><span><font color="FFFFFF" size="4"> Get One Now üñ±Ô∏è</font></span></strong></a></div></td><td><div class="get_one_now_container" style="text-align:center"><a href="https://www.seeedstudio.com/OV5647-69-1-FOV-Camera-module-for-Raspberry-Pi-3B-4B-p-5484.html" target="_blank" rel="noopener noreferrer" class="get_one_now_item"><strong><span><font color="FFFFFF" size="4"> Get One Now üñ±Ô∏è</font></span></strong></a></div></td></tr></table></div><p>This is the recommended device model and is all the hardware that will be used in this tutorial. Of course, if you don&#x27;t have an OV5647 camera on hand, and don&#x27;t have an XIAO, you can use any other CSI Raspberry Pi camera, and any UART-enabled Arduino-enabled development board will do.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="labelled-datasets">Labelled Datasets<a href="#labelled-datasets" class="hash-link" aria-label="Direct link to Labelled Datasets" title="Direct link to Labelled Datasets">‚Äã</a></h2><p>In the contents of this section, we allow users to freely choose the datasets they have. This includes community&#x27;s or their own photos of the scene. This tutorial will introduce the two dominant scenarios. The first one is to use ready-made labelled datasets provided by the <a href="https://roboflow.com/about" target="_blank" rel="noopener noreferrer">Roboflow</a> community. The other is to use high-resolution images that you have taken and labelled the dataset. Please read the different tutorials below according to your needs.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-create-a-free-roboflow-account">Step 1: Create a free Roboflow account<a href="#step-1-create-a-free-roboflow-account" class="hash-link" aria-label="Direct link to Step 1: Create a free Roboflow account" title="Direct link to Step 1: Create a free Roboflow account">‚Äã</a></h3><p>Roboflow provides everything you need to label, train, and deploy computer vision solutions. To get started, create a <a href="https://app.roboflow.com/?ref=blog.roboflow.com" target="_blank" rel="noopener noreferrer">free Roboflow account</a>.</p><p>After reviewing and accepting the terms of service, you will be asked to choose between one of two plans: the Public Plan and the Starter Plan.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/1.png" style="width:800px;height:auto" class="img_ev3q"></div><p>Then, you will be asked to invite collaborators to your workspace. These collaborators can help you annotate images or manage the vision projects in your workspace. Once you have invited people to your workspace (if you want to), you will be able to create a project.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="choose-how-you-get-your-dataset">Choose how you get your dataset<a href="#choose-how-you-get-your-dataset" class="hash-link" aria-label="Direct link to Choose how you get your dataset" title="Direct link to Choose how you get your dataset">‚Äã</a></h3><div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Download Labelled datasets using Roboflow</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Use your own images as a dataset</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><p>Choosing a suitable dataset from Roboflow for direct use involves determining the dataset that best fits the requirements of your project, considering aspects such as the dataset size, quality, relevance, and licensing.</p><p><strong>Step 2. Explore Roboflow Universe</strong></p><p>Roboflow Universe is a platform where you can find various datasets. Visit the Roboflow Universe website and explore the datasets available.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/2.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>Roboflow provides filters and a search function to help you find datasets. You can filter datasets by domain, number of classes, annotation types, and more. Utilize these filters to narrow down the datasets that fit your criteria.</p><p><strong>Step 3. Evaluate Individual Datasets</strong></p><p>Once you have a shortlist, evaluate each dataset individually. Look for:</p><p><strong>Annotation Quality</strong>: Check if the annotations are accurate and consistent.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/3.png" style="width:1000px;height:auto" class="img_ev3q"></div><p><strong>Dataset Size</strong>: Ensure the dataset is large enough for your model to learn effectively but not too large to handle.</p><p><strong>Class Balance</strong>: The dataset should ideally have a balanced number of examples for each class.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/4.png" style="width:1000px;height:auto" class="img_ev3q"></div><p><strong>License</strong>: Review the dataset&#x27;s license to ensure you can use it as intended.</p><p><strong>Documentation</strong>: Review any documentation or metadata that comes with the dataset to better understand its contents and any preprocessing steps that have already been applied.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/5.png" style="width:1000px;height:auto" class="img_ev3q"></div><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>You can find out the condition of the model through <strong><a href="https://docs.roboflow.com/datasets/dataset-health-check" target="_blank" rel="noopener noreferrer">Roboflow Health Check</a></strong>.</p></div></div><p><strong>Step 4. Download the Sample</strong></p><p>If you find the dataset of your choice, then you have the option to download and use it. Roboflow usually allows you to download a sample of the dataset. Test the sample to see if it integrates well with your workflow and if it&#x27;s suitable for your model.</p><p>To continue with the subsequent steps, we recommend that you export the dataset in the format shown below.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/6.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>You&#x27;ll then get the <strong>Raw URL</strong> for this model, keep it safe, we&#x27;ll use that link in the model training step in a bit later.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/26.png" style="width:1000px;height:auto" class="img_ev3q"></div><div class="theme-admonition theme-admonition-caution alert alert--warning admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</div><div class="admonitionContent_S0QG"><p>If you are using Roboflow for the first time and have absolutely no judgement on the selection of datasets, the step of training a model with a dataset to perform an initial test to see the performance may be essential. This can help you gauge whether the dataset will meet your requirements.</p><p>If the dataset meets your requirements and performs well in the initial tests, then it is likely to be suitable for your project. Otherwise, you may need to continue your search or consider expanding the dataset with more images.</p></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><p>Here, I will use the rock-paper-scissors gesture image as a demo to guide you through the tasks of image uploading, labelling and exporting a dataset at Roboflow.</p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>We highly recommend that you use Grove Vision AI V2 to take pictures of your dataset, which is best for Grove Vision AI V2. Grove Vision AI V2 is currently developing the functionality part of taking pictures, and once that is done, you can use Grove Vision AI V2 to capture pictures for your dataset. Until then, you can use photos taken by other devices as datasets.</p></div></div><p><strong>Step 2. Creating a New Project and Uploading images</strong></p><p>Once you&#x27;ve logged into Roboflow, Click on <strong>Create Project</strong>.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/7.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>Name your project (e.g., &quot;Rock-Paper-Scissors&quot;). Define your project as <strong>Object Detection</strong>. Set the <strong>Output Labels</strong> as <strong>Categorical</strong> (since Rock, Paper, and Scissors are distinct categories).</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/8.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>Now it&#x27;s time to upload your hand gesture images.</p><p>Collect images of the rock, paper, and scissors gestures. Ensure you have a variety of backgrounds and lighting conditions. On your project page, click &quot;Add Images&quot;.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/9.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>You can drag and drop your images or select them from your computer. Upload at least 100 images of each gesture for a robust dataset.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/10.png" style="width:1000px;height:auto" class="img_ev3q"></div><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p><strong>How is the dataset size determined?</strong></p><p>It generally depends on a variety of factors: task model, task complexity, data purity, and so on. For example, the human body detection model involves a large number of people, a wide range, the task is more complex, so more data need to be collected.
Another example is the gesture detection model, which only needs to detect the three types of &quot;rock&quot;, &quot;scissors&quot; and &quot;cloth&quot;, and requires fewer categories, so the data set collected is about 500.</p></div></div><p><strong>Step 3: Annotating Images</strong></p><p>After uploading, you&#x27;ll need to annotate the images by labeling the hand gestures.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/11.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>Roboflow offers three different ways of labelling images: Auto Label, Roboflow Labeling and Manual Labeling.</p><ul><li><a href="https://blog.roboflow.com/yolo-world-prompting-tips/" target="_blank" rel="noopener noreferrer"><strong>Auto Label</strong></a>: Use a large generalized model to automatically label images.</li><li><strong>Roboflow Labeling</strong>: Work with a professional team of human labelers. No minimum volumes. No upfront commitments. Bounding Box annotations start at $0.04 and Polygon annotations start at $0.08.</li><li><strong>Manual Labeling</strong>: You and your team label your own images.</li></ul><p>The following describes the most commonly used method of manual labelling.</p><p>Click on &quot;Manual Labeling&quot; button. Roboflow will load the annotation interface.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/12.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>Select the &quot;Start Annotating&quot; button. Draw bounding boxes around the hand gesture in each image.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/13.gif" style="width:1000px;height:auto" class="img_ev3q"></div><p>Label each bounding box as &quot;Rock&quot;, &quot;Paper&quot;, or &quot;Scissors&quot;.</p><p>Use the &quot;&gt;&quot; button to move through your dataset, repeating the annotation process for each image.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/14.gif" style="width:1000px;height:auto" class="img_ev3q"></div><p><strong>Step 4: Review and Edit Annotations</strong></p><p>It&#x27;s essential to ensure annotations are accurate.</p><p>Review each image to make sure the bounding boxes are correctly drawn and labeled. If you find any mistakes, select the annotation to adjust the bounding box or change the label.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/15.png" style="width:1000px;height:auto" class="img_ev3q"></div><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>Incorrect labelling affects the overall performance of the training and can be discarded if some datasets fail to meet the labelling requirements. Here are some bad labelling demonstrations.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/16.png" style="width:700px;height:auto" class="img_ev3q"></div></div></div><p><strong>Step 5: Generating and Exporting the Dataset</strong></p><p>Once all images are annotated. In Annotate click the <strong>Add x images to Dataset</strong> button in the top right corner.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/17.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>Then click the <strong>Add Images</strong> button at the bottom of the new pop-up window.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/18.png" style="width:400px;height:auto" class="img_ev3q"></div><p>Click <strong>Generate</strong> in the left toolbar and click <strong>Continue</strong> in the third <strong>Preprocessing</strong> step.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/19.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>In the <strong>Augmentation</strong> in step 4, select <strong>Mosaic</strong>, which increases generalisation.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/20.png" style="width:1000px;height:auto" class="img_ev3q"></div><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/21.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>In the final <strong>Create</strong> step, please calculate the number of images reasonably according to Roboflow&#x27;s boost; in general, the more images you have, the longer it takes to train the model. However, the more pictures you have will not necessarily make the model more accurate, it mainly depends on whether the dataset is good enough or not.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/22.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>Click on <strong>Create</strong> to create a version of your dataset. Roboflow will process the images and annotations, creating a versioned dataset. After the dataset is generated, click <strong>Export Dataset</strong>. Choose the <strong>COCO</strong> format that matches the requirements of the model you&#x27;ll be training.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/23.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>Click on <strong>Continue</strong> and you&#x27;ll then get the Raw URL for this model. Keep it, we&#x27;ll use the link in the model training step a bit later.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/27.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>Congratulations! You have successfully used Roboflow to upload, annotate, and export a dataset for a Rock-Paper-Scissors hand gesture detection model. With your dataset ready, you can proceed to train a machine learning model using platforms like Google Colab.</p><p>Remember to keep your dataset diverse and well-annotated to improve the accuracy of your future model. Good luck with your model training, and have fun classifying hand gestures with the power of AI!</p></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="training-dataset-exported-model">Training Dataset Exported Model<a href="#training-dataset-exported-model" class="hash-link" aria-label="Direct link to Training Dataset Exported Model" title="Direct link to Training Dataset Exported Model">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-access-the-colab-notebook">Step 1. Access the Colab Notebook<a href="#step-1-access-the-colab-notebook" class="hash-link" aria-label="Direct link to Step 1. Access the Colab Notebook" title="Direct link to Step 1. Access the Colab Notebook">‚Äã</a></h3><p>You can find different kinds of model Google Colab code files on the <a href="https://wiki.seeedstudio.com/ModelAssistant_Introduce_Quick_Start/#model-training" target="_blank" rel="noopener noreferrer">SenseCraft Model Assistant&#x27;s Wiki</a>. If you don&#x27;t know which code you should choose, you can choose any one of them, depending on the class of your model (object detection or image classification).</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/24.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>If you are not already signed into your Google account, please sign in to access the full functionalities of Google Colab.</p><p>Click on &quot;Connect&quot; to allocate resources for your Colab session.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/25.png" style="width:1000px;height:auto" class="img_ev3q"></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-add-your-roboflow-dataset">Step 2. Add your Roboflow Dataset<a href="#step-2-add-your-roboflow-dataset" class="hash-link" aria-label="Direct link to Step 2. Add your Roboflow Dataset" title="Direct link to Step 2. Add your Roboflow Dataset">‚Äã</a></h3><p>Before officially running the code block step-by-step, we need to modify the code&#x27;s content so that the code can use the dataset we prepared. We have to provide a URL to download the dataset directly into the Colab filesystem.</p><p>Please find the <strong>Download the dataset</strong> section in the code. You will see the following sample program.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/28.png" style="width:1000px;height:auto" class="img_ev3q"></div><div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">%mkdir -p Gesture_Detection_Swift-YOLO_192/dataset </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">!wget -c https://universe.roboflow.com/ds/xaMM3ZTeWy?key=5bznPZyI0t -O Gesture_Detection_Swift-YOLO_192/dataset.zip </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">!unzip -q Gesture_Detection_Swift-YOLO_192/dataset.zip -d Gesture_Detection_Swift-YOLO_192/dataset</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>This piece of code is used to create a directory, download a dataset from Roboflow, and unzip it into the newly created directory within a Google Colab environment. Here&#x27;s a breakdown of what each line does:</p><ol><li><p><code>%mkdir -p Gesture_Detection_Swift-YOLO_192/dataset</code>:</p><ul><li>This line creates a new directory called <code>Gesture_Detection_Swift-YOLO_192</code> and a subdirectory called <code>dataset</code>. The <code>-p</code> flag ensures that the command does not return an error if the directory already exists and creates any necessary parent directories.</li></ul></li><li><p><code>!wget -c https://universe.roboflow.com/ds/xaMM3ZTeWy?key=5bznPZyI0t -O Gesture_Detection_Swift-YOLO_192/dataset.zip</code>:</p><ul><li>This line uses <code>wget</code>, a command-line utility, to download the dataset from the provided Roboflow URL. The <code>-c</code> flag allows the download to resume if it gets interrupted. The <code>-O</code> flag specifies the output location and filename for the downloaded file, in this case, <code>Gesture_Detection_Swift-YOLO_192/dataset.zip</code>.</li></ul></li><li><p><code>!unzip -q Gesture_Detection_Swift-YOLO_192/dataset.zip -d Gesture_Detection_Swift-YOLO_192/dataset</code>:</p><ul><li>This line uses the <code>unzip</code> command to extract the contents of the <code>dataset.zip</code> file into the <code>dataset</code> directory that was created earlier. The <code>-q</code> flag runs the <code>unzip</code> command in quiet mode, suppressing most of the output messages.</li></ul></li></ol><p>To customize this code for your own model link from Roboflow:</p><ol><li><p>Replace <code>Gesture_Detection_Swift-YOLO_192</code> with the desired directory name where you want to store your dataset.</p></li><li><p>Replace the Roboflow dataset URL (<code>https://universe.roboflow.com/ds/xaMM3ZTeWy?key=5bznPZyI0t</code>) with the link to your exported dataset (It&#x27;s the Raw URL we got in the <a href="#choose-how-you-get-your-dataset">last step in Labelled Datasets</a>). Make sure to include the key parameter if required for access.</p></li><li><p>Adjust the output filename in the <code>wget</code> command if necessary (<code>-O your_directory/your_filename.zip</code>).</p></li><li><p>Make sure the output directory in the <code>unzip</code> command matches the directory you created and the filename matches the one you set in the <code>wget</code> command.</p></li></ol><div class="theme-admonition theme-admonition-caution alert alert--warning admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</div><div class="admonitionContent_S0QG"><p>If you change the name of a folder directory <code>Gesture_Detection_Swift-YOLO_192</code>, please note that you will need to make changes to other directory names in the code that were used before the change, otherwise an error may occur!</p></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-adjustment-of-model-parameters">Step 3. Adjustment of model parameters<a href="#step-3-adjustment-of-model-parameters" class="hash-link" aria-label="Direct link to Step 3. Adjustment of model parameters" title="Direct link to Step 3. Adjustment of model parameters">‚Äã</a></h3><p>The next step is to adjust the input parameters of the model. Please jump to the Train a model with SSCMA section and you will see the following code snippet.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/29.png" style="width:1000px;height:auto" class="img_ev3q"></div><div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">!sscma.train configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">--cfg-options  \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    work_dir=Gesture_Detection_Swift-YOLO_192 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    num_classes=3 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    epochs=10  \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    height=192 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    width=192 \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data_root=Gesture_Detection_Swift-YOLO_192/dataset/ \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    load_from=Gesture_Detection_Swift-YOLO_192/pretrain.pth</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>This command is used to start the training process of a machine learning model, specifically a YOLO (You Only Look Once) model, using the SSCMA (Seeed Studio SenseCraft Model Assistant) framework. The command includes various options to configure the training process. Here‚Äôs what each part does:</p><ul><li><p><code>!sscma.train</code> is the command to initiate the training within the SSCMA framework.</p></li><li><p><code>configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py</code> specifies the configuration file for the training, which includes settings like the model architecture, training schedule, data augmentation strategies, etc.</p></li><li><p><code>--cfg-options</code> allows you to override the default configurations specified in the <code>.py</code> file with the ones you provide in the command line.</p></li><li><p><code>work_dir=Gesture_Detection_Swift-YOLO_192</code> sets the directory where the training outputs, such as logs and saved model checkpoints, will be stored.</p></li><li><p><code>num_classes=3</code> specifies the number of classes that the model should be trained to recognize. It depends on the number of tags you have, for example rock, paper, scissors should be three tags.</p></li><li><p><code>epochs=10</code> sets the number of training cycles (epochs) to run. Recommended values are between 50 and 100.</p></li><li><p><code>height=192</code> and <code>width=192</code> set the height and width of the input images that the model expects.</p></li><li><p><code>data_root=Gesture_Detection_Swift-YOLO_192/dataset/</code> defines the path to the directory where the training data is located.</p></li><li><p><code>load_from=Gesture_Detection_Swift-YOLO_192/pretrain.pth</code> provides the path to a pre-trained model checkpoint file from which training should resume or use as a starting point for transfer learning.</p></li></ul><p>To customize this command for your own training, you would:</p><ol><li><p>Replace <code>configs/swift_yolo/swift_yolo_tiny_1xb16_300e_coco.py</code> with the path to your own configuration file if you have a custom one.</p></li><li><p>Change <code>work_dir</code> to the directory where you want your training outputs to be saved.</p></li><li><p>Update <code>num_classes</code> to match the number of classes in your own dataset. It depends on the number of tags you have, for example rock, paper, scissors should be three tags.</p></li><li><p>Adjust <code>epochs</code> to the desired number of training epochs for your model. Recommended values are between 50 and 100.</p></li><li><p>Set <code>height</code> and <code>width</code> to match the dimensions of the input images for your model. </p></li></ol><div class="theme-admonition theme-admonition-caution alert alert--warning admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</div><div class="admonitionContent_S0QG"><p>We don&#x27;t really recommend that you change the image size in the Colab code, as this value is a more appropriate dataset size that we have verified to be a combination of size, accuracy, and speed of inference. If you are using a dataset that is not of this size, and you may want to consider changing the image size to ensure accuracy, then please do not exceed 240x240.</p></div></div><ol start="6"><li><p>Change <code>data_root</code> to point to the root directory of your dataset.</p></li><li><p>If you have a different pre-trained model file, update the <code>load_from</code> path accordingly.</p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-4-run-the-google-colab-code">Step 4. Run the Google Colab code<a href="#step-4-run-the-google-colab-code" class="hash-link" aria-label="Direct link to Step 4. Run the Google Colab code" title="Direct link to Step 4. Run the Google Colab code">‚Äã</a></h3><p>The way to run the code block is to click on the play button in the upper left corner of the code block.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/30.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>The code block will be executed after you click the button, and if all goes well, you&#x27;ll see the sign that the code block execution is complete - a tick symbol appears to the left of the block. As shown in the figure is the effect after the execution of the first code block is completed.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/31.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>If you encounter the same error message as mine in the image above, please check that you are using a <strong>T4 GPU</strong>, please <strong>do not use CPU</strong> for this project.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/32.png" style="width:400px;height:auto" class="img_ev3q"></div><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/33.png" style="width:600px;height:auto" class="img_ev3q"></div><p>Then, re-execute the code block. For the first code block, if all goes well, you&#x27;ll see the result shown below.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/34.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>Next, execute all the code blocks from <strong>Download the pretrain model weights file</strong> to <strong>Export the model</strong>. And please make sure that each code block is free of errors.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/36.png" style="width:400px;height:auto" class="img_ev3q"></div><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>Warnings that appear in the code can be ignored.</p></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-5-evaluate-the-model">Step 5. Evaluate the model<a href="#step-5-evaluate-the-model" class="hash-link" aria-label="Direct link to Step 5. Evaluate the model" title="Direct link to Step 5. Evaluate the model">‚Äã</a></h3><p>When you get to the <strong>Evaluate the model</strong> section, you have the option of executing the <strong>Evaluate the TFLite INT8 model</strong> code block.</p><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>Evaluating the TFLite INT8 model involves testing the quantized model&#x27;s predictions against a separate testing dataset to measure its accuracy and performance metrics, assessing the impact of quantization on the model&#x27;s precision, and profiling its inference speed and resource usage to ensure it meets the deployment constraints for edge devices.</p></div></div><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/35.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>The following snippet is the valid part of the result after I executed this code block.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"> Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.450</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.929</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.361</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.474</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.456</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.515</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.529</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.529</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.536</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.537</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">03/19 01:38:43 - mmengine - INFO - bbox_mAP_copypaste: 0.450 0.929 0.361 -1.000 0.474 0.456</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">{&#x27;coco/bbox_mAP&#x27;: 0.45, &#x27;coco/bbox_mAP_50&#x27;: 0.929, &#x27;coco/bbox_mAP_75&#x27;: 0.361, &#x27;coco/bbox_mAP_s&#x27;: -1.0, &#x27;coco/bbox_mAP_m&#x27;: 0.474, &#x27;coco/bbox_mAP_l&#x27;: 0.456}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">FPS: 128.350449 fram/s</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The evaluation results include a series of Average Precision (AP) and Average Recall (AR) metrics, calculated for different Intersection over Union (IoU) thresholds and object sizes, which are commonly used to assess the performance of object detection models.</p><ol><li><p><strong>AP@<!-- -->[IoU=0.50:0.95 | area=all | maxDets=100]<!-- --> = 0.450</strong></p><ul><li>This score is the model&#x27;s average precision across a range of IoU thresholds from 0.50 to 0.95, incremented by 0.05. An AP of 0.450 indicates that your model has moderate accuracy across this range. This is a key metric commonly used for the COCO dataset.</li></ul></li><li><p><strong>AP@<!-- -->[IoU=0.50 | area=all | maxDets=100]<!-- --> = 0.929</strong></p><ul><li>At an IoU threshold of 0.50, the model achieves a high average precision of 0.929, suggesting that it detects objects very accurately under a more lenient matching criterion.</li></ul></li><li><p><strong>AP@<!-- -->[IoU=0.75 | area=all | maxDets=100]<!-- --> = 0.361</strong></p><ul><li>With a stricter IoU threshold of 0.75, the model&#x27;s average precision drops to 0.361, indicating a decline in performance under tighter matching criteria.</li></ul></li><li><p><strong>AP@<!-- -->[IoU=0.50:0.95 | area=small/medium/large | maxDets=100]</strong></p><ul><li>The AP scores vary for objects of different sizes. However, the AP for small objects is -1.000, which could indicate a lack of evaluation data for small objects or poor model performance on small object detection. The AP scores for medium and large objects are 0.474 and 0.456, respectively, suggesting that the model detects medium and large objects relatively better.</li></ul></li><li><p><strong>AR@<!-- -->[IoU=0.50:0.95 | area=all | maxDets=1/10/100]</strong></p><ul><li>The average recall rates for different <code>maxDets</code> values are quite consistent, ranging from 0.515 to 0.529, indicating that the model reliably retrieves most of the true positive instances.</li></ul></li><li><p><strong>FPS: 128.350449 fram/s</strong></p><ul><li>The model processes images at a very fast rate of approximately 128.35 frames per second during inference, indicating potential for real-time or near-real-time applications.</li></ul></li></ol><p>Overall, the model performs excellently at an IoU of 0.50 and moderately at an IoU of 0.75. It performs better on medium and large object detection but may have issues with detecting small objects. Additionally, the model infers at a high speed, making it suitable for scenarios that require fast processing. If detecting small objects is critical in an application, we may need to further optimize the model or collect more small object data to improve performance.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-6-download-the-exported-model-file">Step 6. Download the exported model file<a href="#step-6-download-the-exported-model-file" class="hash-link" aria-label="Direct link to Step 6. Download the exported model file" title="Direct link to Step 6. Download the exported model file">‚Äã</a></h3><p>After the <strong>Export the model</strong> section, you will get the model files in various formats, which will be stored in the ModelAssistant folder by default. In this tutorial, the stored directory is <strong>Gesture_Detection_Swift_YOLO_192</strong>.</p><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>Sometimes Google Colab does not automatically refresh the contents of a folder. In this case you may need to refresh the file directory by clicking the refresh icon in the top left corner.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/39.png" style="width:500px;height:auto" class="img_ev3q"></div></div></div><p>In the directory above, the <strong>.tflite</strong> model files are available for XIAO ESP32S3 and Grove Vision AI V2. For Grove Vision AI V2, be sure to select the model file that uses the <strong>xxx_int8_vela.tflite</strong> format. No other format can be used by Grove Vision AI V2.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/38.png" style="width:400px;height:auto" class="img_ev3q"></div><p>Once you have found the model files, please download them locally to your computer as soon as possible, Google Colab may empty your storage directory if you are idle for a long time!</p><p>So with the steps carried out here, we have successfully exported model files that can be supported by Grove Vision AI V2, next let&#x27;s deploy the model to the device.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="upload-models-via-sensecraft-model-assistant">Upload models via SenseCraft Model Assistant<a href="#upload-models-via-sensecraft-model-assistant" class="hash-link" aria-label="Direct link to Upload models via SenseCraft Model Assistant" title="Direct link to Upload models via SenseCraft Model Assistant">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-7-upload-custom-model-to-grove-vision-ai-v2">Step 7. Upload Custom model to Grove Vision AI V2<a href="#step-7-upload-custom-model-to-grove-vision-ai-v2" class="hash-link" aria-label="Direct link to Step 7. Upload Custom model to Grove Vision AI V2" title="Direct link to Step 7. Upload Custom model to Grove Vision AI V2">‚Äã</a></h3><p>Next, we come to the Model Assistant page.</p><div class="get_one_now_container" style="text-align:center"><a href="https://seeed-studio.github.io/SenseCraft-Web-Toolkit/#/setup/process" target="_blank" rel="noopener noreferrer" class="get_one_now_item"><strong><span><font color="FFFFFF" size="4">Model Assistant üñ±Ô∏è</font></span></strong></a></div><br><p>Please connect the device after selecting Grove Vision AI V2 and then select <strong>Upload Custom AI Model</strong> at the bottom of the page.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/40.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>You will then need to prepare the name of the model, the model file, and the labels. I want to highlight here how this element of the label ID is determined.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/41.png" style="width:500px;height:auto" class="img_ev3q"></div><p><strong>If you are downloading Roboflow&#x27;s dataset directly</strong></p><p>If you downloaded Roboflow&#x27;s dataset directly, then you can view the different categories and its order on the Health Check page. Just install the order entered here.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/42.png" style="width:1000px;height:auto" class="img_ev3q"></div><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>You don&#x27;t need to fill in the numbers in <strong>ID:Object</strong>, just fill in the category name directly, the numbers and colons in front of the categories on the image are added automatically.</p></div></div><p><strong>If you are using a custom dataset</strong></p><p>If you are using a custom dataset, then you can view the different categories and its order on the Health Check page. Just install the order entered here.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/43.png" style="width:1000px;height:auto" class="img_ev3q"></div><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>You don&#x27;t need to fill in the numbers in <strong>ID:Object</strong>, just fill in the category name directly, the numbers and colons in front of the categories on the image are added automatically.</p></div></div><p>Then click Send Model in the bottom right corner. This may take about 3 to 5 minutes or so. If all goes well, then you can see the results of your model in the Model Name and Preview windows above.</p><div style="text-align:center"><img loading="lazy" src="https://files.seeedstudio.com/wiki/visionai_v2_train_model/44.png" style="width:1000px;height:auto" class="img_ev3q"></div><p>Having made it this far, congratulations, you have been able to successfully train and deploy a model of your own.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="common-protocols-and-applications-of-the-model">Common protocols and applications of the model<a href="#common-protocols-and-applications-of-the-model" class="hash-link" aria-label="Direct link to Common protocols and applications of the model" title="Direct link to Common protocols and applications of the model">‚Äã</a></h2><p>During the process of uploading a custom model, in addition to the model files that we can visualise being uploaded, there is also the firmware of the device that needs to be transferred to the device. In the device&#x27;s firmware, there is a set of established communication protocols that specify the format of the model results output, and what the user can do with the models.</p><p>Due to space issues, we won&#x27;t be expanding on the specifics of these protocols in this wiki, we&#x27;ll be detailing this section through documentation on Github. If you are interested in more in-depth development, please go here.</p><div class="get_one_now_container" style="text-align:center"><a href="https://github.com/Seeed-Studio/SSCMA-Micro/blob/dev/docs/protocol/at_protocol.md" target="_blank" rel="noopener noreferrer" class="get_one_now_item"><strong><span><font color="FFFFFF" size="4">SenseCraft Protocols</font></span></strong></a></div><br><p>If you want to continue to use Arduino devices such as the XIAO to realise your prototypes to the true, please refer to the Arduino example program here.</p><div class="get_one_now_container" style="text-align:center"><a href="https://wiki.seeedstudio.com/grove_vision_ai_v2_software_support/#demo-1-use-xiao-to-get-recognition-results" target="_blank" rel="noopener noreferrer" class="get_one_now_item"><strong><span><font color="FFFFFF" size="4">Arduino Exampleüñ±Ô∏è</font></span></strong></a></div><br><h2 class="anchor anchorWithStickyNavbar_LWe7" id="troubleshooting">Troubleshooting<a href="#troubleshooting" class="hash-link" aria-label="Direct link to Troubleshooting" title="Direct link to Troubleshooting">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-what-if-i-follow-the-steps-and-end-up-with-less-than-satisfactory-model-results">1. What if I follow the steps and end up with less than satisfactory model results?<a href="#1-what-if-i-follow-the-steps-and-end-up-with-less-than-satisfactory-model-results" class="hash-link" aria-label="Direct link to 1. What if I follow the steps and end up with less than satisfactory model results?" title="Direct link to 1. What if I follow the steps and end up with less than satisfactory model results?">‚Äã</a></h3><p>If your model&#x27;s recognition accuracy is unsatisfactory, you could diagnose and improve it by considering the following aspects:</p><ol><li><p><strong>Data Quality and Quantity</strong></p><ul><li><strong>Issue</strong>: The dataset might be too small or lack diversity, or there could be inaccuracies in the annotations.</li><li><strong>Solution</strong>: Increase the size and diversity of the training data, and perform data cleaning to correct any annotation errors.</li></ul></li><li><p><strong>Training Process</strong></p><ul><li><strong>Issue</strong>: Training time might be insufficient, or the learning rate could be improperly set, preventing the model from learning effectively.</li><li><strong>Solution</strong>: Increase the number of training epochs, adjust the learning rate and other hyperparameters, and implement early stopping to avoid overfitting.</li></ul></li><li><p><strong>Class Imbalance</strong></p><ul><li><strong>Issue</strong>: Some classes have significantly more samples than others, leading to model bias towards the majority class.</li><li><strong>Solution</strong>: Use class weights, oversample the minority classes, or undersample the majority classes to balance the data.</li></ul></li></ol><p>By thoroughly analyzing and implementing targeted improvements, you can progressively enhance your model&#x27;s accuracy. Remember to use a validation set to test the performance of the model after each modification to ensure the effectiveness of your improvements.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-why-do-i-see-invoke-failed-message-in-sensecraft-deployment-after-following-the-steps-in-the-wiki">2. Why do I see <strong>Invoke failed</strong> message in SenseCraft deployment after following the steps in the Wiki?<a href="#2-why-do-i-see-invoke-failed-message-in-sensecraft-deployment-after-following-the-steps-in-the-wiki" class="hash-link" aria-label="Direct link to 2-why-do-i-see-invoke-failed-message-in-sensecraft-deployment-after-following-the-steps-in-the-wiki" title="Direct link to 2-why-do-i-see-invoke-failed-message-in-sensecraft-deployment-after-following-the-steps-in-the-wiki">‚Äã</a></h3><p>If you encounter an Invoke failed, then you have trained a model that does not meet the requirements for use with the device. Please focus on the following areas.</p><ol><li><p>Please check whether you have modified the image size of Colab. The default compression size is <strong>192x192</strong>, Grove Vision AI V2 requires the image size to be compressed as square, please do not use non-square size for compression. Also don&#x27;t use too large size <em>(no more than 240x240 is recommended)</em>.</p></li><li><p>The model file for Grove Vision AI V2 must be suffixed with <strong>int8_vela.tflite</strong>. Please do not use model files in other formats. This includes <strong>int8.tflite, which is also not available</strong> for Grove Vision AI V2.</p></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="tech-support--product-discussion">Tech Support &amp; Product Discussion<a href="#tech-support--product-discussion" class="hash-link" aria-label="Direct link to Tech Support &amp; Product Discussion" title="Direct link to Tech Support &amp; Product Discussion">‚Äã</a></h2><p>Thank you for choosing our products! We are here to provide you with different support to ensure that your experience with our products is as smooth as possible. We offer several communication channels to cater to different preferences and needs.</p><div class="table-center"><div class="button_tech_support_container"><a href="https://forum.seeedstudio.com/" target="_blank" rel="noopener noreferrer" class="button_forum"></a><a href="https://www.seeedstudio.com/contacts" target="_blank" rel="noopener noreferrer" class="button_email"></a></div><div class="button_tech_support_container"><a href="https://discord.gg/eWkprNDMU7" target="_blank" rel="noopener noreferrer" class="button_discord"></a><a href="https://github.com/Seeed-Studio/wiki-documents/discussions/69" target="_blank" rel="noopener noreferrer" class="button_discussion"></a></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Sensor/Grove/Grove_Sensors/AI-powered/Grove-vision-ai-v2/grove_vision_ai_v2_sscma.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2024-03-12T00:00:00.000Z">Mar 12, 2024</time></b> by <b>Citric</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Grove-vision-ai-v2-camera-supported/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">External Camera supported</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/grove_vision_ai_v2_himax_sdk/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Developing Grove Vision AI V2 using Himax SDK</div></a></nav></div><div>Loading Comments...</div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#materials-required" class="table-of-contents__link toc-highlight">Materials Required</a></li><li><a href="#labelled-datasets" class="table-of-contents__link toc-highlight">Labelled Datasets</a><ul><li><a href="#step-1-create-a-free-roboflow-account" class="table-of-contents__link toc-highlight">Step 1: Create a free Roboflow account</a></li><li><a href="#choose-how-you-get-your-dataset" class="table-of-contents__link toc-highlight">Choose how you get your dataset</a></li></ul></li><li><a href="#training-dataset-exported-model" class="table-of-contents__link toc-highlight">Training Dataset Exported Model</a><ul><li><a href="#step-1-access-the-colab-notebook" class="table-of-contents__link toc-highlight">Step 1. Access the Colab Notebook</a></li><li><a href="#step-2-add-your-roboflow-dataset" class="table-of-contents__link toc-highlight">Step 2. Add your Roboflow Dataset</a></li><li><a href="#step-3-adjustment-of-model-parameters" class="table-of-contents__link toc-highlight">Step 3. Adjustment of model parameters</a></li><li><a href="#step-4-run-the-google-colab-code" class="table-of-contents__link toc-highlight">Step 4. Run the Google Colab code</a></li><li><a href="#step-5-evaluate-the-model" class="table-of-contents__link toc-highlight">Step 5. Evaluate the model</a></li><li><a href="#step-6-download-the-exported-model-file" class="table-of-contents__link toc-highlight">Step 6. Download the exported model file</a></li></ul></li><li><a href="#upload-models-via-sensecraft-model-assistant" class="table-of-contents__link toc-highlight">Upload models via SenseCraft Model Assistant</a><ul><li><a href="#step-7-upload-custom-model-to-grove-vision-ai-v2" class="table-of-contents__link toc-highlight">Step 7. Upload Custom model to Grove Vision AI V2</a></li></ul></li><li><a href="#common-protocols-and-applications-of-the-model" class="table-of-contents__link toc-highlight">Common protocols and applications of the model</a></li><li><a href="#troubleshooting" class="table-of-contents__link toc-highlight">Troubleshooting</a><ul><li><a href="#1-what-if-i-follow-the-steps-and-end-up-with-less-than-satisfactory-model-results" class="table-of-contents__link toc-highlight">1. What if I follow the steps and end up with less than satisfactory model results?</a></li><li><a href="#2-why-do-i-see-invoke-failed-message-in-sensecraft-deployment-after-following-the-steps-in-the-wiki" class="table-of-contents__link toc-highlight">2. Why do I see <strong>Invoke failed</strong> message in SenseCraft deployment after following the steps in the Wiki?</a></li></ul></li><li><a href="#tech-support--product-discussion" class="table-of-contents__link toc-highlight">Tech Support &amp; Product Discussion</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Navigation</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Getting_Started/">Getting Started</a></li><li class="footer__item"><a class="footer__link-item" href="/Sensor_Network/">Sensor and Sensing</a></li><li class="footer__item"><a class="footer__link-item" href="/Network/">Network</a></li><li class="footer__item"><a class="footer__link-item" href="/Edge_Computing/">Edge Computing</a></li><li class="footer__item"><a class="footer__link-item" href="/Cloud/">Cloud</a></li><li class="footer__item"><a class="footer__link-item" href="/Solutions/">Solutions</a></li></ul></div><div class="col footer__col"><div class="footer__title">Ecosystem</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.com/invite/QqMgVwHT3X" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord</a></li><li class="footer__item"><a href="https://project.seeedstudio.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Project Hub</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/ecosystem/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Partners</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/distributors.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Distributors</a></li></ul></div><div class="col footer__col"><div class="footer__title">Quick Guide</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.seeedstudio.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Bazzar</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/get_help/HowToGetHelp" target="_blank" rel="noopener noreferrer" class="footer__link-item">How to get help</a></li><li class="footer__item"><a href="https://support.seeedstudio.com/knowledgebase" target="_blank" rel="noopener noreferrer" class="footer__link-item">FAQs</a></li><li class="footer__item"><a href="https://forum.seeedstudio.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Forum</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/get_help/TechnicalSupport" target="_blank" rel="noopener noreferrer" class="footer__link-item">Technical Support</a></li></ul></div><div class="col footer__col"><div class="footer__title">Company</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.seeedstudio.com/about-us/" target="_blank" rel="noopener noreferrer" class="footer__link-item">About Seeed</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/join-us/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Join us</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/contacts" target="_blank" rel="noopener noreferrer" class="footer__link-item">Contact Us</a></li><li class="footer__item"><a href="https://www.seeedstudio.com/blog/2020/04/22/seeed-in-the-news/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Press</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2024 Seeed Studio, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.be8ea93f.js"></script>
<script src="/assets/js/main.5685c800.js"></script>
</body>
</html>